{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Neural Networks\n",
    "\n",
    "We will implement a shallow neural network to classify digits ranging from 0 to 9. The dataset you'll use is quite famous, it's called 'MNIST' http://yann.lecun.com/exdb/mnist/.\n",
    "\n",
    "\n",
    "You might find [this notebook](https://nbviewer.jupyter.org/github/marc-moreaux/Deep-Learning-classes/blob/master/notebooks/dataset_MNIST.ipynb) to be usefull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the dataset in this directory \n",
    "#! wget http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, gzip, numpy, math\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open('./Files/mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f,encoding=\"latin1\")\n",
    "f.close()\n",
    "\n",
    "def to_one_hot(y, n_classes=10): \n",
    "    _y = np.zeros((len(y), n_classes))\n",
    "    _y[np.arange(len(y)), y] = 1\n",
    "    return _y\n",
    "\n",
    "X_train, y_train = train_set[0], train_set[1]\n",
    "X_valid, y_valid = valid_set[0], valid_set[1]\n",
    "X_test,  y_test  = test_set[0],  test_set[1]\n",
    "X_train = np.transpose(X_train)\n",
    "X_test = np.transpose(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Implement a 2 layers NN\n",
    "\n",
    "We will build a 2 layers SNN \n",
    "    - Layer 1 has 20 neurons with a sigmoid activation\n",
    "    - Layer 2 has 10 neurons with a softmax activation\n",
    "    - Loss is Negative Log Likelihood (wich is also the cross entropy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        \n",
    "def softmax(Z):\n",
    "    \"\"\"Z is a vector eg. [1,2,3]\n",
    "    return: the vector softmax(Z) eg. [.09, .24, .67]\n",
    "    \"\"\"\n",
    "    return np.exp(Z) / np.exp(Z).sum(axis=0)\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "np.random.seed(0)\n",
    "inputSize=28\n",
    "neuronSize=20\n",
    "classifierSize=10\n",
    "testSize = 50000\n",
    "W1 = np.random.normal(0,0.1,(inputSize*inputSize,neuronSize))\n",
    "W2 = np.random.normal(0,0.1,(neuronSize,classifierSize))\n",
    "b1 = np.zeros((neuronSize,testSize))\n",
    "b2 = np.zeros((classifierSize,testSize))\n",
    "A1 = np.matmul( np.transpose(W1), X_train ) + b1\n",
    "A2 = sigmoid(A1)\n",
    "alpha=0.0005\n",
    "\n",
    "y_train_b = to_one_hot(y_train, 10).T\n",
    "y_test_b = to_one_hot(y_test, 10).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pred(X, W1, W2, b1, b2):\n",
    "    \"\"\"Explanations ...\n",
    "    Arguments:\n",
    "        X: An input image (as a vector)(shape is <784,1>)\n",
    "    Returns : a vector ???\n",
    "    \n",
    "    We can say that Pk = P(k|X;W) ; k belongs to [0;9]\n",
    "    Then P = softmax(W * transposed(X))\n",
    "    Because of the hidden layers, we have :\n",
    "    A1 = W1 * transpose(X) + b1 (b1 being a bias)\n",
    "    A2 = sigmoid(A1)\n",
    "    z = W2 * transpose(A2) + b2\n",
    "    and P = softmax(z)\n",
    "    \n",
    "    Hence : P = softmax( W2 * transpose[ sigmoid(W1*transpose[X]) ] )   -> the change between () & [] was done to ease reading\n",
    "    \"\"\"\n",
    "    A1 = np.dot(X.T, W1).T + b1\n",
    "    A2 = sigmoid(A1)\n",
    "    Z = np.dot(A2.T, W2).T + b2\n",
    "    P = softmax(Z.T).T\n",
    "    return P   \n",
    "  \n",
    "def loss(P, Y):\n",
    "    \"\"\"Explanations : \n",
    "    Arguments:\n",
    "        P: The prediction vector corresponding to an image (X^s)\n",
    "        Y: The ground truth of an image\n",
    "    Returns: a vector ???\n",
    "    \n",
    "    We want to calculate the prediction : argmax(W) P(Y|X;W) so Y knowing X and the weights.\n",
    "    We develop that into argmax(W) P(0|X;W) , ... , argmax(W) P(9|X;W)\n",
    "    Hence the prediction becomes argmax(W) Π(i=0;9) & Π(j=0;9) OF ( P[i,j] ^ Y[i,j] )\n",
    "    We can apply the natural logarithm function to simplify this to :\n",
    "    L(W) = argmax(W) Σ(i=0;9) & Σ(j=0;9) OF (-Y[i,j]*ln(P[i,j]))\n",
    "    This is the loss function, we want to reduce it.\n",
    "    \"\"\"\n",
    "    YP = -1* np.matmul(Y,np.log(P+0.000000000000001).T)\n",
    "    loss = np.sum(YP)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dW1(P, Y, W2, A2, X):\n",
    "    \"\"\"Explanations \n",
    "    W1 is a weight matrix applied for each pixel (784) to each neuron of the first layer (20)\n",
    "    \n",
    "    Returns: A vector which is the derivative of the loss with respect to W1\n",
    "    \n",
    "    We want to minimize the loss function, so let's calculate dL/dW1 and \n",
    "    decompose it by going from P to W1 on the schema done in class:\n",
    "    \n",
    "    dL/dW1 = dL/dP * dP/dz * dz/dA2 * dA2/dA1 * dA1/dW1 \n",
    "    \n",
    "    dL/dP = Σ(i=0;9) & Σ(j=0;9) OF (-Y[i,j]/P[i,j])\n",
    "    \n",
    "    dP/dz = dP[i]/dz[j] = d/dz[j] * ( exp(z[i])/Σ(k=0;9) OF exp(z[k]) )\n",
    "        Σ(k=0;9) OF exp(z[k]) will be Σ\n",
    "        If i = j\n",
    "            d/dz[j] * P[i] = [exp(z[i]) * Σ * exp(z[j]) * exp(z[i])]/[Σ²]\n",
    "                           = [exp(z[i])/Σ] - [exp(z[i])²/Σ²]\n",
    "                           = softmax(z[i]) - softmax(z[i])² \n",
    "                           = P[i] (1 - P[j]) \n",
    "                (i=j, so we will have the second term according to j for the sake of future explanation)\n",
    "        \n",
    "        If i != j\n",
    "            d/dz[j] * P[i] = [0 * Σ - exp(z[j]) * exp(z[i])]/[Σ²] \n",
    "                    We have 0 since we derive exp(z[i]) by z[j] and i is never equal to j\n",
    "                           = -[exp(z[j])/Σ]*[exp(z[i])/Σ]\n",
    "                           = -P[i]*P[j]\n",
    "                           = P[i] (0 - P[j])\n",
    "                           \n",
    "        So dP/dZ = P[i](δ[i,j] - P[j]) ; δ = {1 if i=j ; 0 otherwise}\n",
    "        So if i=j, dP/dz is positive (P - P², P among [0,1], P>P²) \n",
    "        If it i!=j, dP/dz is negative (-P[i]P[j], P>0)\n",
    "        \n",
    "    dP/dZ = P[i](δ[i,j] - P[j]) ; δ = {1 if i=j ; 0 otherwise}\n",
    "        \n",
    "    dZ/dA2 = W2\n",
    "    \n",
    "    dA2/dA1 = A2/(1 - A2)\n",
    "    \n",
    "    dA1/dW1 = X\n",
    "    \n",
    "    But we can simplify dL/dP * dP/dz :\n",
    "    dL/dz[i] = dL/dP*dP/dz[i] = Σ(j=0;9) OF ( (-Y[i,j]/P[j])*P[i]*(δ[i,j] - P[j]) ) \n",
    "                                #basic aggregation of the previous calculations\n",
    "                        \n",
    "                              = P[i] * Σ(j=0;9) OF ( (-Y[i,j]/P[j])*(δ[i,j] - P[j]) ) \n",
    "                              #We extract P[i] because it does not depend on j\n",
    "                              \n",
    "                              = P[i] * Σ(j=0;9) OF ( ( (-Y[i,j]*δ[i,j]/P[j]) + Y[j]) \n",
    "                              #We distribute Y/P to (δ - P)\n",
    "                              \n",
    "                              = P[i] * [ Σ(j=0;9) OF (-Y[i,j]*δ[i,j]/P[j]) + Σ(j=0;9) OF (Y[j]) ] \n",
    "                              #we divide the sum in 2 parts\n",
    "                              \n",
    "                     We have  Σ(j=0;9) OF (-Y[j]*δ)/P[j]  which is equal to -Y[i]/P[i], i=j, because it is 0 if i!=j\n",
    "                     And Σ(j=0;9) OF (Y[j]) is equal to 1 since Y is that type of vector : [0,1,0,0,0,0,0,0,0,0]\n",
    "                              \n",
    "                              = P[i] * [-Y[i]/P[i] + 1] = -Y[i] + P[i]\n",
    "    dL/dz[i] = P[i] - Y[i]\n",
    "                              \n",
    "    So, we can finally deduce that\n",
    "    \n",
    "    dL/dW1 = (P[i] - Y[i]) *  W2  *    A2    *  (1-A2) *     X \n",
    "    SIZE :      [1 * 10]    [10*20]  [20 * 1]   [20 * 1]  [1 * 28²]\n",
    "                [1 * 10]          [10 * 1]          [20 * 28²]\n",
    "                        [1 * 1]              [20 * 28²]\n",
    "      It is a vector of size  [20,28²].\n",
    "      \n",
    "N.B. : When calculating size, we overpass the transpose problem and simply verify the dimension.\n",
    "        \"\"\"\n",
    "    \n",
    "    PY = np.subtract(P,Y)\n",
    "\n",
    "    A2_1_A2 = np.matmul(A2.T,(np.ones(A2.shape)-A2))\n",
    "\n",
    "    \n",
    "    PYW = np.matmul(PY.T,W2.T)\n",
    "    \n",
    "    PYW_A21A2 = np.matmul(PYW,A2_1_A2)\n",
    "    \n",
    "    dW1 = np.matmul(X,PYW_A21A2)\n",
    "\n",
    "    \n",
    "    return dW1\n",
    "\n",
    "def db1(P, Y, W2, A2):\n",
    "    \"\"\"Explanations \n",
    "    The b1 are the biases applied to the first layer of neuron (20,1)\n",
    "    \n",
    "    Arguments:\n",
    "        L is the loss af a sample (a scalar)\n",
    "    Returns: A scalar which is the derivative of the Loss with respect to b1\n",
    "    \n",
    "    dL/db1 = dL/dP * dP/dz * dz/dA2 * dA2/dA1 * dA1/db1\n",
    "        That is basically equal to dL/dW1, except that the last term was dA1/dW1 = X, now it is dA1/db1 = 1\n",
    "    So dL/dB1 = (P[i] - Y[i]) *  W2  *     A2 *    (1-A2)  , we don't need L as an argument though\n",
    "    SIZE :        [1 * 10]    [10*20]  [20 * 1]   [20 * 1]\n",
    "                        [1 * 20]            [20 * 20]\n",
    "      It is a matrix of size [1,20] despite the comment before.\n",
    "    \"\"\"\n",
    "    \n",
    "    PY = np.subtract(P,Y)\n",
    "    \n",
    "    A2_1_A2 = np.matmul(A2,(1-A2).T)\n",
    "    PYW = np.matmul(PY.T,W2.T)\n",
    "    \n",
    "    db1 = np.matmul(PYW,A2_1_A2)\n",
    "    \n",
    "    return db1\n",
    "\n",
    "\n",
    "def dW2(P, Y, A2):\n",
    "    \"\"\"Explanations \n",
    "    W2 is a weight matrix too, it is applied for each neuron from the first layer (20) to each neuron from the second layer (10)\n",
    "\n",
    "    With the same logic, let's calculate dL/dW2 :\n",
    "    \n",
    "    dL/dW2 = dL/dP * dP/dz * dz/dW2\n",
    "           = (P[i] - Y[i]) *   A2\n",
    "    SIZE :    [10 * 1]      [1 * 20]\n",
    "    dL/dW2 is a vector of size [10,20].\n",
    "    \"\"\"\n",
    "    PY = np.subtract(P,Y)\n",
    "    dW2 = np.matmul(PY,A2.T)\n",
    "    return dW2\n",
    "\n",
    "def db2(P, Y):\n",
    "    \"\"\"Explanations \n",
    "    The b2 are the biases applied to the first layer of neuron (10,1)\n",
    "    \n",
    "    Same logic,\n",
    "    \n",
    "    dL/db2 = dL/dP * dP/dz * dz/db2\n",
    "           = (P[i]-Y[i]) * 1\n",
    "           = P[i] - Y[i]\n",
    "    So it is a vector of size [10,1]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    db2 = np.subtract(P,Y)\n",
    "    return db2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.57904561e-05   2.26239616e-05   2.24090588e-05 ...,   1.51675938e-05\n",
      "    2.24053014e-05   1.51675914e-05]\n",
      " [  1.93878143e-05   2.00316682e-05   2.00298886e-05 ...,   1.93965132e-05\n",
      "    2.00228602e-05   1.93965102e-05]\n",
      " [  2.16220775e-05   1.90733536e-05   1.90805181e-05 ...,   2.24790655e-05\n",
      "    1.90798627e-05   2.24790635e-05]\n",
      " ..., \n",
      " [  1.76610553e-05   2.08362890e-05   2.08347154e-05 ...,   1.81350793e-05\n",
      "    2.08329256e-05   1.81350800e-05]\n",
      " [  2.80387417e-05   1.57911141e-05   1.57961955e-05 ...,   2.96888603e-05\n",
      "    1.57967855e-05   2.96888585e-05]\n",
      " [  1.91624088e-05   2.02982871e-05   2.03042213e-05 ...,   1.94167813e-05\n",
      "    2.02966211e-05   1.94167746e-05]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(0,20): #instead of doing a for loop we should use the loss function and test if the difference between 2 iterations is less important than a little threshold\n",
    "    #Forward propagation: we compute the probability of being of each of the 10 classes\n",
    "    P = Pred(X_train, W1, W2, b1, b2)\n",
    "    \n",
    "    #Backward propagation: we update the parameters\n",
    "    A1 = np.matmul( np.transpose(W1), X_train ) + b1\n",
    "    A2 = sigmoid(A1)\n",
    "    b2 = b2 - alpha*db2(P,y_train_b)\n",
    "    #W1 = W1 - alpha*dW1(P,y_train_b,W2,A2,X_train)\n",
    "    b1 = b1 - alpha*db1(P,y_train_b,W2,A2).T\n",
    "    #W2 = W2 - alpha*dW2(P,y_train_b,A2).T\n",
    "\n",
    "print(P)\n",
    "\n",
    "#As we can see our method is not working here, we commented W1 and W2 to avoid the computation to be too much. \n",
    "#Our try to do ever computation using matricial computation did not fully suceeded (it create overflow here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of the accuracy of the model on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (20,10000) (20,50000) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b56871b8691c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#We designed our test function but could not use it since our training did not fully suceeded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcpt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msizeTest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msizeTest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-969a7d8f22ad>\u001b[0m in \u001b[0;36mPred\u001b[0;34m(X, W1, W2, b1, b2)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mHence\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mW2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m   \u001b[1;33m->\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mchange\u001b[0m \u001b[0mbetween\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[0mwas\u001b[0m \u001b[0mdone\u001b[0m \u001b[0mto\u001b[0m \u001b[0mease\u001b[0m \u001b[0mreading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mA1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mA2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (20,10000) (20,50000) "
     ]
    }
   ],
   "source": [
    "#We designed our test function but could not use it since our training did not fully suceeded\n",
    "y_pred=Pred(X_test, W1, W2, b1, b2)\n",
    "cpt=0\n",
    "sizeTest=10000\n",
    "for i in range (0,sizeTest):\n",
    "    if ((y_test_b[:,i].argmax(axis=0))==y_pred[:,i].argmax(axis=0)):\n",
    "        cpt=cpt+1\n",
    "accuracy=(cpt/sizeTest)*100\n",
    "print(\"Accuracy % = \", accuracy)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
