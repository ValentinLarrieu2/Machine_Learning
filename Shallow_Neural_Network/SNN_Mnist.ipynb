{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Neural Networks\n",
    "\n",
    "We will implement a shallow neural network to classify digits ranging from 0 to 9. The dataset you'll use is quite famous, it's called 'MNIST' http://yann.lecun.com/exdb/mnist/.\n",
    "\n",
    "\n",
    "You might find [this notebook](https://nbviewer.jupyter.org/github/marc-moreaux/Deep-Learning-classes/blob/master/notebooks/dataset_MNIST.ipynb) to be usefull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the dataset in this directory \n",
    "#! wget http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle, gzip, numpy, math\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open('./Files/mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f,encoding=\"latin1\")\n",
    "f.close()\n",
    "\n",
    "def to_one_hot(y, n_classes=10): \n",
    "    _y = np.zeros((len(y), n_classes))\n",
    "    _y[np.arange(len(y)), y] = 1\n",
    "    return _y\n",
    "\n",
    "X_train, y_train = train_set[0], train_set[1]\n",
    "X_valid, y_valid = valid_set[0], valid_set[1]\n",
    "X_test,  y_test  = test_set[0],  test_set[1]\n",
    "\n",
    "\n",
    "# We need to transform the output y from a number of 0 to 9, to a vector of 10 boolean values\n",
    "y_train = to_one_hot(y_train)\n",
    "y_valid = to_one_hot(y_valid)\n",
    "y_test = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Implement a 2 layers NN\n",
    "\n",
    "We will build a 2 layers SNN \n",
    "    - Layer 1 has 20 neurons with a sigmoid activation\n",
    "    - Layer 2 has 10 neurons with a softmax activation\n",
    "    - Loss is Negative Log Likelihood (wich is also the cross entropy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        \n",
    "def softmax(Z):\n",
    "    \"\"\"Z is a vector eg. [1,2,3]\n",
    "    return: the vector softmax(Z) eg. [.09, .24, .67]\n",
    "    \"\"\"\n",
    "    return np.exp(Z) / np.exp(Z).sum(axis=0)\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "np.random.seed(0)\n",
    "inputSize=28\n",
    "neuronSize=20\n",
    "classifierSize=10\n",
    "testSize = 50000\n",
    "\n",
    "W1, b1 = np.random.normal(size=(inputSize*inputSize, neuronSize)),  np.random.normal(size=neuronSize)\n",
    "W2, b2 = np.random.normal(size=(neuronSize, classifierSize)), np.random.normal(size=classifierSize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pred(X, W1, W2, b1, b2):\n",
    "    \"\"\"Explanations ...\n",
    "    Arguments:\n",
    "        X: An input image (as a vector)(shape is <784,1>)\n",
    "    Returns : a vector ???\n",
    "    \n",
    "    We can say that Pk = P(k|X;W) ; k belongs to [0;9]\n",
    "    Then P = softmax(W * transposed(X))\n",
    "    Because of the hidden layers, we have :\n",
    "    A1 = W1 * transpose(X) + b1 (b1 being a bias)\n",
    "    A2 = sigmoid(A1)\n",
    "    z = W2 * transpose(A2) + b2\n",
    "    and P = softmax(z)\n",
    "    \n",
    "    Hence : P = softmax( W2 * transpose[ sigmoid(W1*transpose[X]) ] )   -> the change between () & [] was done to ease reading\n",
    "    \"\"\"\n",
    "    A1 = np.dot(W1.T, X) + b1\n",
    "    A2 = sigmoid(A1)\n",
    "    Z = np.dot(W2.T,A2) + b2\n",
    "    P = softmax(Z)\n",
    "    return P   \n",
    "  \n",
    "def loss(P, Y):\n",
    "    \"\"\"Explanations : \n",
    "    Arguments:\n",
    "        P: The prediction vector corresponding to an image (X^s)\n",
    "        Y: The ground truth of an image\n",
    "    Returns: a vector ???\n",
    "    \n",
    "    We want to calculate the prediction : argmax(W) P(Y|X;W) so Y knowing X and the weights.\n",
    "    We develop that into argmax(W) P(0|X;W) , ... , argmax(W) P(9|X;W)\n",
    "    Hence the prediction becomes argmax(W) Π(i=0;9) & Π(j=0;9) OF ( P[i,j] ^ Y[i,j] )\n",
    "    We can apply the natural logarithm function to simplify this to :\n",
    "    L(W) = argmax(W) Σ(i=0;9) & Σ(j=0;9) OF (-Y[i,j]*ln(P[i,j]))\n",
    "    This is the loss function, we want to reduce it.\n",
    "    \"\"\"\n",
    "    ##############################(- sum(Y * np.log(P)))\n",
    "    YP = -1* np.matmul(Y,np.log(P+0.000000000000001).T)\n",
    "    loss = np.sum(YP)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dW1(X, Y, P, W2, W1, b1):\n",
    "    \"\"\"Explanations \n",
    "    W1 is a weight matrix applied for each pixel (784) to each neuron of the first layer (20)\n",
    "    \n",
    "    Returns: A vector which is the derivative of the loss with respect to W1\n",
    "    \n",
    "    We want to minimize the loss function, so let's calculate dL/dW1 and \n",
    "    decompose it by going from P to W1 on the schema done in class:\n",
    "    \n",
    "    dL/dW1 = dL/dP * dP/dz * dz/dA2 * dA2/dA1 * dA1/dW1 \n",
    "    \n",
    "    dL/dP = Σ(i=0;9) & Σ(j=0;9) OF (-Y[i,j]/P[i,j])\n",
    "    \n",
    "    dP/dz = dP[i]/dz[j] = d/dz[j] * ( exp(z[i])/Σ(k=0;9) OF exp(z[k]) )\n",
    "        Σ(k=0;9) OF exp(z[k]) will be Σ\n",
    "        If i = j\n",
    "            d/dz[j] * P[i] = [exp(z[i]) * Σ * exp(z[j]) * exp(z[i])]/[Σ²]\n",
    "                           = [exp(z[i])/Σ] - [exp(z[i])²/Σ²]\n",
    "                           = softmax(z[i]) - softmax(z[i])² \n",
    "                           = P[i] (1 - P[j]) \n",
    "                (i=j, so we will have the second term according to j for the sake of future explanation)\n",
    "        \n",
    "        If i != j\n",
    "            d/dz[j] * P[i] = [0 * Σ - exp(z[j]) * exp(z[i])]/[Σ²] \n",
    "                    We have 0 since we derive exp(z[i]) by z[j] and i is never equal to j\n",
    "                           = -[exp(z[j])/Σ]*[exp(z[i])/Σ]\n",
    "                           = -P[i]*P[j]\n",
    "                           = P[i] (0 - P[j])\n",
    "                           \n",
    "        So dP/dZ = P[i](δ[i,j] - P[j]) ; δ = {1 if i=j ; 0 otherwise}\n",
    "        So if i=j, dP/dz is positive (P - P², P among [0,1], P>P²) \n",
    "        If it i!=j, dP/dz is negative (-P[i]P[j], P>0)\n",
    "        \n",
    "    dP/dZ = P[i](δ[i,j] - P[j]) ; δ = {1 if i=j ; 0 otherwise}\n",
    "        \n",
    "    dZ/dA2 = W2\n",
    "    \n",
    "    dA2/dA1 = A2/(1 - A2)\n",
    "    \n",
    "    dA1/dW1 = X\n",
    "    \n",
    "    But we can simplify dL/dP * dP/dz :\n",
    "    dL/dz[i] = dL/dP*dP/dz[i] = Σ(j=0;9) OF ( (-Y[i,j]/P[j])*P[i]*(δ[i,j] - P[j]) ) \n",
    "                                #basic aggregation of the previous calculations\n",
    "                        \n",
    "                              = P[i] * Σ(j=0;9) OF ( (-Y[i,j]/P[j])*(δ[i,j] - P[j]) ) \n",
    "                              #We extract P[i] because it does not depend on j\n",
    "                              \n",
    "                              = P[i] * Σ(j=0;9) OF ( ( (-Y[i,j]*δ[i,j]/P[j]) + Y[j]) \n",
    "                              #We distribute Y/P to (δ - P)\n",
    "                              \n",
    "                              = P[i] * [ Σ(j=0;9) OF (-Y[i,j]*δ[i,j]/P[j]) + Σ(j=0;9) OF (Y[j]) ] \n",
    "                              #we divide the sum in 2 parts\n",
    "                              \n",
    "                     We have  Σ(j=0;9) OF (-Y[j]*δ)/P[j]  which is equal to -Y[i]/P[i], i=j, because it is 0 if i!=j\n",
    "                     And Σ(j=0;9) OF (Y[j]) is equal to 1 since Y is that type of vector : [0,1,0,0,0,0,0,0,0,0]\n",
    "                              \n",
    "                              = P[i] * [-Y[i]/P[i] + 1] = -Y[i] + P[i]\n",
    "    dL/dz[i] = P[i] - Y[i]\n",
    "                              \n",
    "    So, we can finally deduce that\n",
    "    \n",
    "    dL/dW1 = (P[i] - Y[i]) *  W2  *    A2    *  (1-A2) *     X \n",
    "    SIZE :      [1 * 10]    [10*20]  [20 * 1]   [20 * 1]  [1 * 28²]\n",
    "                [1 * 10]          [10 * 1]          [20 * 28²]\n",
    "                        [1 * 1]              [20 * 28²]\n",
    "      It is a vector of size  [20,28²].\n",
    "      \n",
    "N.B. : When calculating size, we overpass the transpose problem and simply verify the dimension.\n",
    "        \"\"\"\n",
    "    \n",
    "    A1 = np.dot(W1.T, X) + b1\n",
    "    A2 = sigmoid(A1)\n",
    "    \n",
    "    dW1 = np.matrix(np.dot(np.dot((P - Y), W2.transpose()), (A2 * (1 - A2)).transpose()) * X).transpose()\n",
    "\n",
    "    \"\"\"PY = np.subtract(P,Y)\n",
    "\n",
    "    A2_1_A2 = np.matmul(A2.T,(np.ones(A2.shape)-A2))\n",
    "\n",
    "    \n",
    "    PYW = np.matmul(PY.T,W2.T)\n",
    "    \n",
    "    PYW_A21A2 = np.matmul(PYW,A2_1_A2)\n",
    "    \n",
    "    dW1 = np.matmul(X,PYW_A21A2)\"\"\"\n",
    "\n",
    "    \n",
    "    return dW1\n",
    "\n",
    "def db1(X, Y, P, W2, W1, b1):\n",
    "    \"\"\"Explanations \n",
    "    The b1 are the biases applied to the first layer of neuron (20,1)\n",
    "    \n",
    "    Arguments:\n",
    "        L is the loss af a sample (a scalar)\n",
    "    Returns: A scalar which is the derivative of the Loss with respect to b1\n",
    "    \n",
    "    dL/db1 = dL/dP * dP/dz * dz/dA2 * dA2/dA1 * dA1/db1\n",
    "        That is basically equal to dL/dW1, except that the last term was dA1/dW1 = X, now it is dA1/db1 = 1\n",
    "    So dL/dB1 = (P[i] - Y[i]) *  W2  *     A2 *    (1-A2)  , we don't need L as an argument though\n",
    "    SIZE :        [1 * 10]    [10*20]  [20 * 1]   [20 * 1]\n",
    "                        [1 * 20]            [20 * 20]\n",
    "      It is a matrix of size [1,20] despite the comment before.\n",
    "    \"\"\"\n",
    "    A1 = np.dot(W1.T, X) + b1\n",
    "    A2 = sigmoid(A1)\n",
    "    return np.dot(np.dot((P - Y), W2.transpose()), (A2 * (1 - A2)).transpose())\n",
    "\n",
    "\n",
    "def dW2(X, Y, P, W1, b1):\n",
    "    \"\"\"Explanations \n",
    "    W2 is a weight matrix too, it is applied for each neuron from the first layer (20) to each neuron from the second layer (10)\n",
    "\n",
    "    With the same logic, let's calculate dL/dW2 :\n",
    "    \n",
    "    dL/dW2 = dL/dP * dP/dz * dz/dW2\n",
    "           = (P[i] - Y[i]) *   A2\n",
    "    SIZE :    [10 * 1]      [1 * 20]\n",
    "    dL/dW2 is a vector of size [10,20].\n",
    "    \"\"\"\n",
    "    A1 = np.dot(W1.T, X) + b1\n",
    "    A2 = sigmoid(A1)\n",
    "    return (np.matrix(P - Y).transpose()*A2).transpose()\n",
    "\n",
    "def db2(P, Y):\n",
    "    \"\"\"Explanations \n",
    "    The b2 are the biases applied to the first layer of neuron (10,1)\n",
    "    \n",
    "    Same logic,\n",
    "    \n",
    "    dL/db2 = dL/dP * dP/dz * dz/db2\n",
    "           = (P[i]-Y[i]) * 1\n",
    "           = P[i] - Y[i]\n",
    "    So it is a vector of size [10,1]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    db2 = np.subtract(P,Y)\n",
    "    return db2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number  0\n",
      "Iteration number  1\n",
      "Iteration number  2\n",
      "Iteration number  3\n",
      "Iteration number  4\n",
      "Iteration number  5\n",
      "Iteration number  6\n",
      "Iteration number  7\n",
      "Iteration number  8\n",
      "Iteration number  9\n",
      "Iteration number  10\n",
      "Iteration number  11\n",
      "Iteration number  12\n",
      "Iteration number  13\n",
      "Iteration number  14\n",
      "Iteration number  15\n",
      "Iteration number  16\n",
      "Iteration number  17\n",
      "Iteration number  18\n",
      "Iteration number  19\n",
      "Iteration number  20\n",
      "Iteration number  21\n",
      "Iteration number  22\n",
      "Iteration number  23\n",
      "Iteration number  24\n",
      "Iteration number  25\n",
      "Iteration number  26\n",
      "Iteration number  27\n",
      "Iteration number  28\n",
      "Iteration number  29\n",
      "Iteration number  30\n",
      "Iteration number  31\n",
      "Iteration number  32\n",
      "Iteration number  33\n",
      "Iteration number  34\n",
      "Iteration number  35\n",
      "Iteration number  36\n",
      "Iteration number  37\n",
      "Iteration number  38\n",
      "Iteration number  39\n",
      "Iteration number  40\n",
      "Iteration number  41\n",
      "Iteration number  42\n",
      "Iteration number  43\n",
      "Iteration number  44\n",
      "Iteration number  45\n",
      "Iteration number  46\n",
      "Iteration number  47\n",
      "Iteration number  48\n",
      "Iteration number  49\n"
     ]
    }
   ],
   "source": [
    "m = X_train.shape[0] # number of samples\n",
    "alpha = 0.1 # learning rate\n",
    "\n",
    "nbIter = 50\n",
    "# Optimization loop\n",
    "\"\"\"With a training dataset as big as this one (50000 samples), the computation is extremely long.\n",
    "   For this reason, we can only loop over 10 iterations so it doesn't take too long. (10 minutes)\n",
    "   But the results aren't very good since the algorithm does not have enough iterations to optimize the parameters well enough.\n",
    "\"\"\"\n",
    "for i in range(nbIter):\n",
    "    print(\"Iteration number \",i)\n",
    "\n",
    "    # Make prediction for each sample with current model paramaters\n",
    "    P = np.array([Pred(x, W1, W2, b1, b2) for x in X_train])\n",
    "\n",
    "    # Compute the global dataset value for the derivative of the loss with respect to each parameter.\n",
    "    # This is achieved by computing the average of the derivatives for all the samples.\n",
    "    dW1_global = (1/m) * sum(np.array([dW1(X_train[i], y_train[i], P[i], W2, W1, b1) for i in range(m)]))\n",
    "    db1_global = (1/m) * sum(np.array([db1(X_train[i], y_train[i], P[i], W2, W1, b1) for i in range(m)]))\n",
    "    dW2_global = (1/m) * sum(np.array([dW2(X_train[i], y_train[i], P[i], W1, b1) for i in range(m)]))\n",
    "    db2_global = (1/m) * sum(np.array([db2(y_train[i], P[i]) for i in range(m)]))\n",
    "\n",
    "    # Update model parameters using standard gradient descent\n",
    "    W1 = W1 - (alpha * dW1_global)\n",
    "    b1 = b1 - (alpha * db1_global)\n",
    "    W2 = W2 - (alpha * dW2_global)\n",
    "    b2 = b2 - (alpha * db2_global)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of the accuracy of the model on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per sample: [ 3.59642753  6.56135011  0.65771282 ...,  3.52756302  3.82871969\n",
      "  1.11072133]\n",
      "Total loss: 33464.474734111915\n",
      "\n",
      "Comparison of actual and predicted output for first 25 test samples (for visualization):\n",
      "Actual output:     [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4]\n",
      "Predicted output:  [6 6 1 6 6 1 6 1 1 6 6 1 6 6 6 1 6 6 6 1 6 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test of our first model (optimized using all training samples)\n",
    "\"\"\"\n",
    "m_test =  X_test.shape[0] # number of test samples\n",
    "\n",
    "# Compute predictions of output for test set using computed optimized model parameters\n",
    "P_test = np.array([Pred(x, W1, W2, b1, b2) for x in X_test])\n",
    "\n",
    "# Compute loss of model parameters for each sample\n",
    "loss_global = np.array([loss(P_test[i], y_test[i]) for i in range(m_test)])\n",
    "# Compute total loss\n",
    "loss_total = sum(loss_global)\n",
    "\n",
    "# For better visualization of results,\n",
    "# transform the outputs and predictions into actual numbers, using argmax (index of max probability)\n",
    "y_test_argmax = np.array([np.argmax(y_test[i]) for i in range(m_test)])\n",
    "P_test_argmax = np.array([np.argmax(P_test[i]) for i in range(m_test)])\n",
    "\n",
    "print(\"Loss per sample: {}\".format(loss_global))\n",
    "print(\"Total loss: {}\".format(loss_total))\n",
    "print(\"\\nComparison of actual and predicted output for first 25 test samples (for visualization):\")\n",
    "print(\"Actual output:     {}\".format(y_test_argmax[:25]))\n",
    "print(\"Predicted output:  {}\".format(P_test_argmax[:25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Since the computations took too long when we took into account all 50000 training samples, \n",
    "    we tried to optimize the parameters using only a sample of the training set, reducing its size from 50000 to 1000.\n",
    "    This optimization took about 16 minutes.\n",
    "\"\"\"\n",
    "nbIter = 1500\n",
    "batch_size = 1500\n",
    "\n",
    "# Take only first 1000 samples from traing set\n",
    "X_sample = X_train[:batch_size]\n",
    "y_sample = y_train[:batch_size]\n",
    "\n",
    "m_sample = X_sample.shape[0] #number of samples\n",
    "\n",
    "#Initialize new parameters\n",
    "W1_sample, b1_sample = np.random.normal(size=(inputSize*inputSize, neuronSize)),  np.random.normal(size=neuronSize)\n",
    "W2_sample, b2_sample = np.random.normal(size=(neuronSize, classifierSize)), np.random.normal(size=classifierSize)\n",
    "\n",
    "# Optimization loop\n",
    "for i in range(nbIter):\n",
    "    # Make prediction for each sample with current model paramaters\n",
    "    P_sample = np.array([Pred(x, W1_sample, W2_sample, b1_sample, b2_sample) for x in X_sample])\n",
    "\n",
    "    # Compute the global dataset value for the derivative of the loss with respect to each parameter.\n",
    "    # This is achieved by computing the average of the derivatives for all the samples.\n",
    "    dW1_global_sample = (1/m_sample) * sum(np.array([dW1(X_sample[i], y_sample[i], P_sample[i], W2_sample, W1_sample, b1_sample) for i in range(m_sample)]))\n",
    "    db1_global_sample = (1/m_sample) * sum(np.array([db1(X_sample[i], y_sample[i], P_sample[i], W2_sample, W1_sample, b1_sample) for i in range(m_sample)]))\n",
    "    dW2_global_sample = (1/m_sample) * sum(np.array([dW2(X_sample[i], y_sample[i], P_sample[i], W1_sample, b1_sample) for i in range(m_sample)]))\n",
    "    db2_global_sample = (1/m_sample) * sum(np.array([db2(y_sample[i], P_sample[i]) for i in range(m_sample)]))\n",
    "\n",
    "    # Update model parameters using standard gradient descent\n",
    "    W1_sample = W1_sample - (alpha * dW1_global_sample)\n",
    "    b_sample1 = b1_sample - (alpha * db1_global_sample)\n",
    "    W2_sample = W2_sample - (alpha * dW2_global_sample)\n",
    "    b2_sample = b2_sample - (alpha * db2_global_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: [ 0.15080078  1.04671801  0.68424589 ...,  1.97371365  2.1148837\n",
      "  0.64010547]\n",
      "Total loss: 14799.092644300494\n",
      "\n",
      "Comparison of actual and predicted output for first 25 test samples (for visualization):\n",
      "Actual output:     [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4]\n",
      "Predicted output:  [7 2 1 0 4 1 9 3 4 7 0 1 7 0 9 5 8 7 2 5 7 8 9 4 4]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test of our second model (optimized using only a sample of the training set)\n",
    "\"\"\"\n",
    "\n",
    "m_test =  X_test.shape[0] # number of test samples\n",
    "\n",
    "# Compute predictions of output for test set using computed optimized model parameters\n",
    "P_sample_test = np.array([Pred(x, W1_sample, W2_sample, b1_sample, b2_sample)for x in X_test])\n",
    "\n",
    "# Compute loss of model parameters for each sample\n",
    "loss_sample_global = np.array([loss(P_sample_test[i], y_test[i]) for i in range(m_test)])\n",
    "# Compute total loss\n",
    "loss_sample_total = sum(loss_sample_global)\n",
    "\n",
    "# For better visualization of results,\n",
    "# transform the outputs and predictions into actual numbers, using argmax (index of max probability)\n",
    "y_test_argmax = np.array([np.argmax(y_test[i]) for i in range(m_test)])\n",
    "P_sample_test_argmax = np.array([np.argmax(P_sample_test[i]) for i in range(m_test)])\n",
    "\n",
    "print(\"Loss: {}\".format(loss_sample_global))\n",
    "print(\"Total loss: {}\".format(loss_sample_total))\n",
    "print(\"\\nComparison of actual and predicted output for first 25 test samples (for visualization):\")\n",
    "print(\"Actual output:     {}\".format(y_test_argmax[:25]))\n",
    "print(\"Predicted output:  {}\".format(P_sample_test_argmax[:25]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
