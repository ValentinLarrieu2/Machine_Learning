{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@author: Valentin Larrieu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBFqSEkKqpCN"
   },
   "source": [
    "# RNN on IMDB\n",
    "# Using Many-to-One for movie rating predicton\n",
    "\n",
    "\n",
    "### Objective:\n",
    "We will implement two different networks to perform automatic rating (0 or 1) of a movie given the text of its review.\n",
    "We will use the ```imdb``` (internet movie database) dataset.\n",
    "\n",
    "The reviews are already available in the form of indexes that point to a word dictionary: each word is already encoded as an index in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmkCSNaXLqjh"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T07:29:14.662517Z",
     "start_time": "2019-02-20T07:29:03.226969Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "AOqjzDwioJj9"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from tensorflow.python.keras.datasets import imdb\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.layers import Dense, Activation, Embedding, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from tensorflow.python.keras import Model\n",
    "from tensorflow.python.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5Yp4OQVvUtr"
   },
   "source": [
    "## Parameters of the model\n",
    "\n",
    "-  We only consider the ```top_words``` first words in the word dictionary\n",
    "- We truncate/zerp-pad each sequence a length ```max_review_length````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T07:29:14.721253Z",
     "start_time": "2019-02-20T07:29:14.711279Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4C_Pv7rYvRkM"
   },
   "outputs": [],
   "source": [
    "top_words = 5000 \n",
    "max_review_length = 100\n",
    "INDEX_FROM = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsNcRimyLzgP"
   },
   "source": [
    "## Import IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T07:29:28.806611Z",
     "start_time": "2019-02-20T07:29:14.759972Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5Gfe1ex8oN8Q"
   },
   "outputs": [],
   "source": [
    "# Import the IMDB data and only consider the ``top_words``` most used words\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words, index_from=INDEX_FROM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSc5LmksOLyr"
   },
   "source": [
    "## Data content\n",
    "\n",
    "- ```X_train``` and ```X_test``` are numpy arrays of lists. \n",
    "  - each item in a list is the index in the word dictionary. So that a list is the sequence of index of words.\n",
    "\n",
    "- ```y_train``` and ```y_test``` are a numpy arrays of the same dimension as ```X_train``` and ```X_test``` \n",
    "  - they contains the values 0 (bad movie) or 1 (good movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T07:29:30.504194Z",
     "start_time": "2019-02-20T07:29:28.871437Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "colab_type": "code",
    "id": "WouODCPrtiuu",
    "outputId": "39e8a6d4-be43-43c4-f2f7-a6280b305508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X_train): <class 'numpy.ndarray'>\n",
      "number of training sequences: X_train.shape: (25000,)\n",
      "type(X_train[0]): <class 'list'>\n",
      "length of the first training sequence: len(X_train[0]): 218\n",
      "length of the second training sequence: len(X_train[0]): 189\n",
      "list of data of the first training sequence: X_train[0]: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "maximum length of a training sequence: 2494\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFKCAYAAADScRzUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"type(X_train):\", type(X_train))\n",
    "print(\"number of training sequences: X_train.shape:\", X_train.shape)\n",
    "print(\"type(X_train[0]):\",type(X_train[0]))\n",
    "print(\"length of the first training sequence: len(X_train[0]):\",len(X_train[0]))\n",
    "print(\"length of the second training sequence: len(X_train[0]):\",len(X_train[1]))\n",
    "print(\"list of data of the first training sequence: X_train[0]:\", X_train[0] )\n",
    "len_list = [len(train) for train in X_train]\n",
    "print(\"maximum length of a training sequence:\", max(len_list))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(len_list, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmdV_XKp1viX"
   },
   "source": [
    "## Details of how the reviews are encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T07:29:36.935885Z",
     "start_time": "2019-02-20T07:29:30.600213Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "pAkcOsnB1vig",
    "outputId": "a1fcfa5b-f2cd-4210-9988-a902e6d61acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> although i had seen <UNK> in a theater way back in <UNK> i couldn't remember anything of the plot except for vague images of kurt thomas running and fighting against a backdrop of stone walls and disappointment regarding the ending br br after reading some of the other reviews i picked up a copy of the newly released dvd to once again enter the world of <UNK> br br it turns out this is one of those films produced during the <UNK> that would go directly to video today the film stars <UNK> <UNK> kurt thomas as jonathan <UNK> <UNK> out of the blue to <UNK> the nation of <UNK> to enter and hopefully win the game a <UNK> <UNK> <UNK> by the khan who <UNK> his people by yelling what sounds like <UNK> power the goal of the mission involves the star wars defense system jonathan is trained in the martial arts by princess <UNK> who never speaks or leaves the house once trained tries to blend in with the <UNK> by wearing a bright red <UNK> with <UNK> of blue and white needless to say <UNK> finds himself running and fighting for his life along the stone streets of <UNK> on his way to a date with destiny and the game br br star kurt thomas was ill served by director robert <UNK> who it looks like was never on the set the so called script is just this side of incompetent see other reviews for the many <UNK> throughout the town of <UNK> has a few good moments but is ultimately ruined by bad editing the ending <UNK> still there's the <UNK> of a good action adventure here a hong kong version with more <UNK> action and faster pace might even be pretty good\n"
     ]
    }
   ],
   "source": [
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in X_train[1000] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T07:29:37.360974Z",
     "start_time": "2019-02-20T07:29:37.327836Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Hfl42LGCugWB",
    "outputId": "f8e456e5-c6bf-4a4c-b175-0d9f035bce18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(y_train): <class 'numpy.ndarray'>\n",
      "y_train.shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"type(y_train):\", type(y_train))\n",
    "print(\"y_train.shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T07:29:37.682971Z",
     "start_time": "2019-02-20T07:29:37.668926Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "iVw65PNNuobX",
    "outputId": "9603104d-d8f8-408a-8bb9-8abfae882ab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (25000,)\n",
      "y_test.shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V18OA7oQNH3c"
   },
   "source": [
    "## Data processing\n",
    "\n",
    "Sequences (represented as a list of values) in ```X_train``` represent the reviews.\n",
    "They can have different length.\n",
    "To train the network we should modify them so that they all have the same length.\n",
    "We do this by:\n",
    "- truncating the ones that are too long\n",
    "- padding-with-zero them the ones that are too short.\n",
    "\n",
    "This is obtained using ```sequence.pad_sequences``` of keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T07:35:45.711864Z",
     "start_time": "2019-02-20T07:35:45.665986Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "colab_type": "code",
    "id": "JhmiHsOGoRwT",
    "outputId": "4b25066d-26a8-4a15-f06c-da3dcc24710a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train[0]): 100\n",
      "len(X_train[1]): 100\n",
      "X_train[0]: [1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
      "    2    8    4  107  117    2   15  256    4    2    7 3766    5  723\n",
      "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
      "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
      "    2   18    4  226   22   21  134  476   26  480    5  144   30    2\n",
      "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
      "   88   12   16  283    5   16 4472  113  103   32   15   16    2   19\n",
      "  178   32]\n"
     ]
    }
   ],
   "source": [
    "# truncate and pad input sequences\n",
    "\n",
    "# CODE-RNN1-1\n",
    "# --- START CODE HERE\n",
    "# We set xtrain and xtest\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# --- END CODE HERE\n",
    "\n",
    "print(\"len(X_train[0]):\", len(X_train[0]))\n",
    "print(\"len(X_train[1]):\", len(X_train[1]))\n",
    "print(\"X_train[0]:\", X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlrDTuk5K65Q"
   },
   "source": [
    "## First model\n",
    "\n",
    "In the first model, we will simply \n",
    "- learn a word embedding  (```Embedding``` layer in keras) and apply it to each of item of a sequence, \n",
    "  -  in keras, embedding is not a matrix going from one-hot-encoding to embedding, it is a layer that goes from index-in-word-dictionary to embedding\n",
    "  - the embedding goes from ```top_words``` dimensions to  ```embedding_vector_length``` dimensions\n",
    "- average the embedding obtained for each word of a seuqnece over all words of the sequence (you should use ```K.mean``` and ```Lambda``` from the keras backend)\n",
    "- apply a fully connected (```Dense``` layer in keras) which output activation is a sigmoid ()predicting the 0 or 1 rating)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T07:34:07.744546Z",
     "start_time": "2019-02-20T07:34:07.712635Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ufW00TGcs3Jj"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786
    },
    "colab_type": "code",
    "id": "zspaUptgtW9l",
    "outputId": "514dc159-0d52-4edb-a182-084a2dbd2b8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Querying Tensorflow master (grpc://10.68.71.154:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1214412658676765023)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 12923923415013130710)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 14023717436323019964)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2332390434200345092)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6601266510620753786)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1693512971540063515)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6617084153076787564)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 4044509543469222105)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 14673585727794350605)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10268864489896820685)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 9837007920802047385)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vector_length = 32\n",
    "\n",
    "# CODE-RNN1-2\n",
    "# --- START CODE HERE\n",
    "# We use TPU for faster execution\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model =  tf.contrib.tpu.keras_to_tpu_model(\n",
    "   model,\n",
    "   strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "       tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "           tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "   )\n",
    ")\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# We create our model\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1,keepdims=False)))\n",
    "model.add(Dense(1,  activation='sigmoid'))\n",
    "# --- END CODE HERE\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "pFXz4AS6tawQ",
    "outputId": "d98f34ac-9670-4f73-a649-ae603a1ff080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 3s 105us/sample - loss: 0.6399 - acc: 0.7294 - val_loss: 0.5642 - val_acc: 0.7753\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.4881 - acc: 0.8139 - val_loss: 0.4417 - val_acc: 0.8182\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 2s 89us/sample - loss: 0.3951 - acc: 0.8434 - val_loss: 0.3855 - val_acc: 0.8360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6510e49940>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and fit the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBqyzLJRUIsC"
   },
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wYH-9VV-1vk-",
    "outputId": "99541024-831f-411e-e657-fd06a6150b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.60%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOj0xZFb1vl-"
   },
   "source": [
    "## Using the trained embedding to find equivalence between words\n",
    "\n",
    "Since the embedding is part of the models, we can look at the trained embedding matrix $E$ and use it to get the most similar words (according to the trained matrix $E$) in the dictionary.\n",
    "Use the weights of the ```Embedding``` layer to find the most similar words to ```amazing```. We will use an Euclidean distance for that.\n",
    "- Retrieve the weights of the ```Embedding layer```\n",
    "- Get the position of ```amazing``` in the dictionary\n",
    "- Get the word-embedding of ```amazing```\n",
    "- Find (using Euclidean distance), the closest embedded-words to ```amazing```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "8QhLjj-b1vmB",
    "outputId": "7b53bbb8-1035-46b5-96b2-404804687a19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: \n",
      " [[-0.05179723  0.00850957 -0.0391795  ... -0.04039804 -0.03847113\n",
      "   0.0094984 ]\n",
      " [-0.08929677  0.0586829  -0.07911096 ... -0.09703251  0.05919173\n",
      "  -0.01311124]\n",
      " [ 0.09090492 -0.07743029  0.02932649 ...  0.07603452 -0.00361073\n",
      "   0.06172582]\n",
      " ...\n",
      " [-0.05261633  0.00711187 -0.05368832 ... -0.0105042  -0.01780583\n",
      "  -0.03694503]\n",
      " [ 0.00509162  0.0022678   0.05407919 ...  0.02712802  0.02099186\n",
      "   0.0648945 ]\n",
      " [ 0.18236518 -0.1697735   0.1612339  ...  0.16513574 -0.21081875\n",
      "   0.19849889]]\n",
      "id amazing: \n",
      " 480\n",
      "weight amazing: \n",
      " [-0.6308524   0.6502041  -0.6465216  -0.70949376 -0.7601271  -0.767558\n",
      "  0.63946354  0.7426589   0.8333374   0.6583711  -0.6971321   0.7374126\n",
      " -0.7618154   0.69683766  0.6283307   0.6566646  -0.7365796  -0.64067614\n",
      " -0.7736393   0.7128281  -0.7314594   0.67765045 -0.7733566  -0.64349914\n",
      " -0.72323775  0.74359494 -0.6729522   0.6367253  -0.6699486  -0.7664627\n",
      "  0.63663125 -0.71732885]\n",
      "['8', 'best', 'superb', 'wonderful', 'highly']\n"
     ]
    }
   ],
   "source": [
    "# CODE-RNN1-3\n",
    "# --- START CODE HERE\n",
    "\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "print(\"weights: \\n\", weights)\n",
    "\n",
    "id_amazing = word_to_id['amazing']\n",
    "print(\"id amazing: \\n\", id_amazing)\n",
    "\n",
    "weight_amazing = weights[id_amazing,:]\n",
    "print(\"weight amazing: \\n\", weight_amazing)\n",
    "\n",
    "# We set hw much words we want to get\n",
    "top_words_nb = 5\n",
    "\n",
    "# We compute the distance\n",
    "distance = numpy.mean((weights - weight_amazing)**2, axis = 1)\n",
    "\n",
    "# We sort the element with respect to their distance\n",
    "indexes = numpy.argsort(distance)\n",
    "\n",
    "# We get the top 5 words\n",
    "top_words_list = [id_to_word[k] for k in indexes[1:top_words_nb+1]]\n",
    "print(top_words_list)\n",
    "\n",
    "# --- END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zK9e5Eo1Ks2a"
   },
   "source": [
    "## Second model\n",
    "\n",
    "In the second model, we will replace\n",
    "- average the obtained embedding over the sequence (use ```K.mean``` and ```Lambda```from keras backend)\n",
    "- by a RNN layer (more precisely an ```LSTM```) in a Many-To-One configuration with $n_a=100$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwoXuOqqVDOy"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "id": "7dl-CSMKoViX",
    "outputId": "2209e9da-4286-43aa-ba0d-dfe1c257810c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Querying Tensorflow master (grpc://10.68.71.154:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1214412658676765023)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 12923923415013130710)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 14023717436323019964)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2332390434200345092)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6601266510620753786)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1693512971540063515)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6617084153076787564)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 4044509543469222105)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 14673585727794350605)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10268864489896820685)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 9837007920802047385)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "\n",
    "# CODE-RNN1-4\n",
    "# --- START CODE HERE\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model =  tf.contrib.tpu.keras_to_tpu_model(\n",
    "   model,\n",
    "   strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "       tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "           tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "   )\n",
    ")\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "\n",
    "# This time we use a LSTM and not a lambda\n",
    "model.add(LSTM(max_review_length)) \n",
    "\n",
    "model.add(Dense(1,  activation='sigmoid'))\n",
    "# --- END CODE HERE\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "-bp7PzX7oXtB",
    "outputId": "6da65b63-f47a-49d0-d091-07d3f2cab2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 47s 2ms/sample - loss: 0.4513 - acc: 0.7757 - val_loss: 0.3549 - val_acc: 0.8438\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 45s 2ms/sample - loss: 0.3033 - acc: 0.8750 - val_loss: 0.3696 - val_acc: 0.8358\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 45s 2ms/sample - loss: 0.2690 - acc: 0.8906 - val_loss: 0.4013 - val_acc: 0.8366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f650359bcf8>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and fit the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1LN_fjMWBHJ"
   },
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RlMEKRbzoavm",
    "outputId": "02e42c00-0066-4d7e-bda9-3916df15bd91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.66%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TPRNNimbdforstudents_Larrieu.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
