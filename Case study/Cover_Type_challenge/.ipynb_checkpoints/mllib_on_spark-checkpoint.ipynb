{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@author: Valentin Larrieu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier,RandomForestClassifier                              \n",
    "from pyspark.ml.regression import DecisionTreeRegressor,RandomForestRegressor\n",
    "from pyspark.sql import SQLContext\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Please look at the report for more details.\n",
    "# My way to do this competition was to test different models, improve them then select the best one and try to pefect it.\n",
    "# I tried some feature selection and cleaning. For instance rebalancing the dataset since all the cover type are not equaly balanced, but it did not iprove the score. I suppose its a bias linked with how\n",
    "# The cover type are balanded also in the test set (some cover have 10 times more datapoints than others).\n",
    "# After trying the logistic regression model i implemented a cross validation to optimise parameters. I saw that the result could be improved so i tried other algorithms.\n",
    "# I used random forest then decision tree. This one gave me the best result (regarding to the limit of databricks) on Mllib.\n",
    "# In order to improve my score i then tried some of the model of sklearn. The best result I found were with the extraTree algorithm. I reached a score of 0.95967 on kaggle with it (code of model 3 in the file \"SDI_701_sklearn_Valentin_Larrieu\").\n",
    "# Databricks limited me on the number of estimator (too much ressources needed), so i trained my model locally on my PC to slightly improve the results (to gain some .0001%).\n",
    "#\n",
    "# You will find in this notebook the code :\n",
    "#      -[Model 1] The best Model I could build with Mllib (A Decision Tree with cross validation)\n",
    "#      -[Model 2] The best Model I could build with sklearn (An Extratree) on Databricks before the improvment i did locally\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We fix the seed\n",
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We import the data\n",
    "df_train = spark.read.  \\\n",
    "         option(\"header\", \"true\"). \\\n",
    "         option(\"nullValue\", \"?\"). \\\n",
    "         option(\"inferSchema\", \"true\"). \\\n",
    "         option(\"sep\", \",\"). \\\n",
    "         csv(\"/FileStore/tables/train_set-51e11.csv\") \n",
    "\n",
    "df_test = spark.read.  \\\n",
    "         option(\"header\", \"true\"). \\\n",
    "         option(\"nullValue\", \"?\"). \\\n",
    "         option(\"inferSchema\", \"true\"). \\\n",
    "         option(\"sep\", \",\"). \\\n",
    "         csv(\"/FileStore/tables/test_set-b5f57.csv\") \n",
    "\n",
    "# We split it according to the split giving us the best results (90% / 10%)\n",
    "trainData, testData = df_train.randomSplit([0.90,0.10],seed=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Train data size: 528720 rows, 56 columns\n",
       "Test data size: 226595 rows, 55 columns\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We control the size of our elements\n",
    "print('Train data size: {} rows, {} columns'.format(df_train.count(), len(df_train.columns)))\n",
    "print('Test data size: {} rows, {} columns'.format(df_test.count(), len(df_test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=[\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\", \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \"Horizontal_Distance_To_Fire_Points\", \"Wilderness_Area1\", \"Wilderness_Area2\", \"Wilderness_Area3\", \"Wilderness_Area4\", \"Soil_Type1\", \"Soil_Type2\", \"Soil_Type3\", \"Soil_Type4\", \"Soil_Type5\", \"Soil_Type6\", \"Soil_Type7\", \"Soil_Type8\", \"Soil_Type9\", \"Soil_Type10\", \"Soil_Type11\", \"Soil_Type12\", \"Soil_Type13\", \"Soil_Type14\", \"Soil_Type15\", \"Soil_Type16\", \"Soil_Type17\", \"Soil_Type18\", \"Soil_Type19\", \"Soil_Type20\", \"Soil_Type21\", \"Soil_Type22\", \"Soil_Type23\", \"Soil_Type24\", \"Soil_Type25\", \"Soil_Type26\", \"Soil_Type27\", \"Soil_Type28\", \"Soil_Type29\", \"Soil_Type30\", \"Soil_Type31\", \"Soil_Type32\", \"Soil_Type33\", \"Soil_Type34\", \"Soil_Type35\", \"Soil_Type36\", \"Soil_Type37\", \"Soil_Type38\", \"Soil_Type39\", \"Soil_Type40\"], outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">(&apos;Train data performance Decision Tree = &apos;, 0.9763744425137119)\n",
       "(&apos;Test data performance Decision Tree = &apos;, 0.9071774741199612)\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############# Model 1 Cross validation and decision tree\n",
    "\n",
    "# Create initial Decision Tree Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"Cover_Type\", featuresCol=\"features\", cacheNodeIds=True)\n",
    "\n",
    "# We create our evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# We set the parameters we want to try\n",
    "paramGridDt = ParamGridBuilder().\\\n",
    "            addGrid(dt.maxDepth, [30]).\\\n",
    "            addGrid(dt.maxBins, [200]).\\\n",
    "            addGrid(dt.impurity, [\"entropy\"]).\\\n",
    "            addGrid(dt.minInstancesPerNode, [3]).\\\n",
    "            build()\n",
    "# Here there is no different choice for each parameter, that is because we optimised the different param\n",
    "\n",
    "# We set the pipeline\n",
    "pipelineDt = Pipeline(stages=[vector_assembler, dt])\n",
    "\n",
    "#Create 5-fold CrossValidator\n",
    "cv3 = CrossValidator(estimator=pipelineDt, estimatorParamMaps=paramGridDt, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "#Fit cross-validation model\n",
    "cvModel3 = cv3.fit(trainData)\n",
    "\n",
    "#Use test set to measure the accuracy of our model on new data\n",
    "#Prediction\n",
    "pred_training_cv3 = cvModel3.transform(trainData)\n",
    "pred_test_cv3 = cvModel3.transform(testData) #test\n",
    "\n",
    "#Evaluation\n",
    "# performance on training data\n",
    "print(\"Train data performance Decision Tree = \", evaluator.evaluate(pred_training_cv3))\n",
    "\n",
    "# performance on test data\n",
    "print(\"Test data performance Decision Tree = \", evaluator.evaluate(pred_test_cv3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We export our best model to submit it on kaggle\n",
    "model = cvModel3\n",
    "\n",
    "# Make predictions on testData\n",
    "predictions = model.transform(df_test) \n",
    "\n",
    "\n",
    "predictions = predictions.withColumn(\"Cover_Type\", predictions[\"prediction\"].cast(\"int\")) \n",
    "\n",
    "# Select columns Id and prediction\n",
    "(predictions\n",
    " .repartition(1)\n",
    " .select('Id', 'Cover_Type')\n",
    " .write\n",
    " .format('com.databricks.spark.csv')\n",
    " .options(header='true')\n",
    " .mode('overwrite')\n",
    " .save('/FileStore/kaggle-submission'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/FileStore/kaggle-submission/_SUCCESS</td><td>_SUCCESS</td><td>0</td></tr><tr><td>dbfs:/FileStore/kaggle-submission/_committed_5702443627356436892</td><td>_committed_5702443627356436892</td><td>199</td></tr><tr><td>dbfs:/FileStore/kaggle-submission/_committed_5868879643784577909</td><td>_committed_5868879643784577909</td><td>199</td></tr><tr><td>dbfs:/FileStore/kaggle-submission/_committed_9174243400005013254</td><td>_committed_9174243400005013254</td><td>199</td></tr><tr><td>dbfs:/FileStore/kaggle-submission/_committed_vacuum4319346291273316749</td><td>_committed_vacuum4319346291273316749</td><td>129</td></tr><tr><td>dbfs:/FileStore/kaggle-submission/_started_9174243400005013254</td><td>_started_9174243400005013254</td><td>0</td></tr><tr><td>dbfs:/FileStore/kaggle-submission/part-00000-tid-9174243400005013254-7650f166-2638-4abb-a192-5506563ab2e5-3896-c000.csv</td><td>part-00000-tid-9174243400005013254-7650f166-2638-4abb-a192-5506563ab2e5-3896-c000.csv</td><td>2039369</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We display the path of the file\n",
    "display(dbutils.fs.ls(\"dbfs:/FileStore/kaggle-submission\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Model 2 : ExtraTrees with Sklearn\n",
    "\n",
    "# We need pandas dataframe for sklearn\n",
    "df_train2 = df_train.toPandas()\n",
    "df_test2 = df_test.toPandas()\n",
    "\n",
    "Y = df_train2.Cover_Type\n",
    "# We drop the ID column because it do not give usefull information\n",
    "X = df_train2.drop(['Id','Cover_Type'],axis=1)\n",
    "X_test_input = df_test2.drop('Id',axis=1)\n",
    "\n",
    "# We split our data\n",
    "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(X,Y,test_size=0.1)\n",
    "\n",
    "# We set our ExtraTrees model with parameters we optimised\n",
    "et = ExtraTreesClassifier(n_estimators=200, criterion= 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">             precision    recall  f1-score   support\n",
       "\n",
       "          1       0.96      0.93      0.94     19162\n",
       "          2       0.94      0.97      0.95     25714\n",
       "          3       0.93      0.96      0.95      3393\n",
       "          4       0.93      0.86      0.90       258\n",
       "          5       0.93      0.74      0.83       896\n",
       "          6       0.92      0.87      0.89      1574\n",
       "          7       0.97      0.94      0.95      1875\n",
       "\n",
       "avg / total       0.94      0.94      0.94     52872\n",
       "\n",
       "(&apos;ExtraTrees Accuracy :&apos;, 0.94445074897866543)\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We fit our model\n",
    "et.fit(X_train,Y_train)\n",
    "\n",
    "# We use it to predict our output\n",
    "Y_hat = et.predict(X_test)\n",
    "\n",
    "# We print the results\n",
    "print(metrics.classification_report(Y_test,Y_hat))\n",
    "print(\"ExtraTrees Accuracy :\", metrics.accuracy_score(Y_test,Y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "Error while obtaining a new communication channel"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We retrain our model with our entire set to have the best model for kaggle\n",
    "et2 = ExtraTreesClassifier(n_estimators=200, criterion= 'entropy')\n",
    "et2.fit(X,Y)\n",
    "Y_hat_export = et2.predict(X_test_input)\n",
    "\n",
    "export_df = pd.DataFrame({'Id':df_test2.Id.values,'Cover_Type':Y_hat_export}).sort_index(ascending=False,axis=1)\n",
    "df_export_2 = sqlContext.createDataFrame(export_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_export_2\n",
    " .repartition(1)\n",
    " .select('Id', 'Cover_Type')\n",
    " .write\n",
    " .format('com.databricks.spark.csv')\n",
    " .options(header='true')\n",
    " .mode('overwrite')\n",
    " .save('/FileStore/kaggle-submission2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We display the path of the file\n",
    "display(dbutils.fs.ls(\"dbfs:/FileStore/kaggle-submission2\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "name": "Kaggle_Larrieu_SD701_Rendu",
  "notebookId": 705710625259203,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
