{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Usefull piece of code </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "pd.options.display.max_columns = 100 \n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can be usefull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T12:34:47.244798Z",
     "start_time": "2019-06-14T12:34:47.238814Z"
    }
   },
   "outputs": [],
   "source": [
    "import math                         \n",
    "import string\n",
    "import random\n",
    "import copy\n",
    "from datetime import timedelta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Flatten, Dropout, Activation, Bidirectional\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback, History\n",
    "#from tensorflow.keras import optimizers\n",
    "from tensorflow.train import RMSPropOptimizer, AdamOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drive Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rarely used\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import gmtime, strftime\n",
    "from scipy.sparse import csc_matrix\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(inputPath, low_memory=False, sep=\";\", encoding='utf-8', dtype='object') #decimal=\",\" , index_col = 0, index_col='DATE', names=[\"tt\",\"ee\"]\n",
    "df = df.fillna(\"\")\n",
    "df_X = df.copy()\n",
    "df_X.drop(columns=['label_true'],inplace=True)\n",
    "a,b = df.loc[0]\n",
    "\n",
    "#df['affair'].hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mess_bs_group = df_mess_train.groupby(['messid'], as_index=False) # group data by message (messid)\n",
    "nb_mess = len(np.unique(df_mess_train['messid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(k, int)\n",
    "np.ones(k, int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.where(nb_tries > 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0['affair'] = (df0.affairs > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = np.argwhere(n_ones>100)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Enroll_Rate'] == df['Enroll_Rate'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Case%20study/Cover_Type_challenge/sklearn_ExtraTrees_model.ipynb\n",
    "\n",
    "# We remove the infinite values the column division could have created\n",
    "X.Shade_mean=X.Shade_mean.map(lambda x: 0 if np.isinf(x) else x)\n",
    "X[X==np.inf] = np.nan\n",
    "X.fillna(X.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_dist.groupby('bsid')['dist'].count()\n",
    "sup = df_dist.groupby('bsid')['dist'].apply(lambda x: (x > distMax).sum())\n",
    "comparaison = pd.DataFrame([count, sup], index = [\"count\", \"nbSup\"]).T\n",
    "comparaison['bsid'] = comparaison.index\n",
    "comparaison[[\"count\", \"nbSup\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_hours = pd.DataFrame(df.groupby('hour')[\"click\"].mean()) # Fill here for the influence of the hour\n",
    "df_hours.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_cols_not_in_df(df,list_col):\n",
    "    '''Return the list of columns from \"list_col\" that are not in the given dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        The dataframe to check columns\n",
    "    list_col : list[string]\n",
    "        The list of column to test if yes or not they are in the columns of the dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cols_not_in_df : list[string]\n",
    "        The list of columns in list_col but not in the columns of df\n",
    "    '''\n",
    "    cols_not_in_df = []\n",
    "    for col in list_col:\n",
    "        if col not in df.columns:\n",
    "            cols_not_in_df.append(col)\n",
    "    return cols_not_in_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.random()\n",
    "np.random.randint(k)\n",
    "np.random.choice(np.where(nb_tries == 0)[0])\n",
    "np.random.normal(cum_rewards / (nb_tries + 1), 1. / (nb_tries + 1))\n",
    "np.random.randn(nb)\n",
    "np.random.shuffle(ind)\n",
    "np.random.permutation(n_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply a function to a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IAS_brut_log'] = df['IAS_brut'].apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to a column list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://colab.research.google.com/drive/1kQ-dbxF0vD-cSF3cZctWnDmHpngY9QAN#scrollTo=4gQvZc8D-hgr\n",
    "\"\"\"\n",
    "    Apply the method given as argument to the column specified of the dataframe\n",
    "    \n",
    "    @ df : the dataframe on which we want to apply the methods\n",
    "    @ list_col : the list of colums on which we want to apply the method\n",
    "    @ method : the method we want to apply\n",
    "\n",
    "    => df : dataframe with method applied\n",
    "\"\"\"\n",
    "def apply_to_cols_df(df, list_col, fct):\n",
    "    for col in list_col:\n",
    "        df[col] = df[col].apply(fct)\n",
    "    return df\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a column of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_col_df(df, df_col, col_name = \"added\"):\n",
    "    '''Adds a column \"df_col\" with the name \"col_name\" to the given \"df\" dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        The dataframe to add the column to\n",
    "    df_col : dataframe \n",
    "        The column to add to the dataframe\n",
    "    col_name : string\n",
    "        The name of the column to add\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DF : dataframe\n",
    "        The dataframe with the new column df_col added\n",
    "    '''\n",
    "    DF = copy.deepcopy(df)\n",
    "    \n",
    "    if col_name:\n",
    "        df_col.name = col_name\n",
    "\n",
    "    DF = pd.merge(DF, df_col, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    DF = DF.interpolate()\n",
    "    DF = DF.fillna(0)\n",
    "\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join with a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Creates a new column of the dataset corresponding to the equivalent value in the corresponding table :\n",
    "    ex : the code \"FRA\" could becomes \"France\" with the corresponding table\n",
    "    \n",
    "    @ df : the dataframe in which we want to create the new column\n",
    "    @ df_corresp : the correspence dataframe  with a second column which is the new label we want\n",
    "    @ column_name : the name of the column in the original dataset\n",
    "    @ index_column_corres : the column in the correspondence dataframe which as same data as the one in the main dataframe\n",
    "    @ index_column_replacer : the column in the correspondence dataframe from which we will put the data in the new column\n",
    "    @ replace_nan : the boolean to choose if we replace nan value by nothing\n",
    "\"\"\"\n",
    "def replace_by_correspondence(df, df_corresp, column_name, index_column_corres = 0, index_column_replacer = 1, replace_nan = False):\n",
    "    column_replacer = df_corresp.columns[index_column_replacer]\n",
    "    column_corres = df_corresp.columns[index_column_corres]\n",
    "    df_corresp = df_corresp.rename(columns={column_corres: column_name}) # we rename the column for the join\n",
    "    dictionary = df_corresp.set_index(column_name).to_dict()[column_replacer]\n",
    "    df[column_name + '_corresp'] = df[column_name].map(dictionary)\n",
    "    # We replace nan values by nothing\n",
    "    if (replace_nan):\n",
    "        df[column_name + '_corresp'] = df[column_name + '_corresp'].fillna(\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train, list_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "print(df.info())\n",
    "df.groupby('affair').mean()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Avazu_case_study/blob/master/Avazu_CaseStudy.ipynb\n",
    "toShow = pd.DataFrame(index = df.columns)\n",
    "\n",
    "for col in df.columns:\n",
    "    toShow.loc[col, 'Column type'] = df[col].dtype\n",
    "    toShow.loc[col, 'Column unique values'] = len(df[col].unique())\n",
    "    \n",
    "toShow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of different element for each feature in Xtrain : \\n\", Xtrain.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_freq = np.sum(df['click'])/len(df['click'])\n",
    "print(\"The click frequency is: \", click_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataframe_variables_distribution(df):\n",
    "    '''Plots the distribution of all the columns of a dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        The dataframe to plot columns distribution from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    '''\n",
    "    plt.figure(figsize=(30,20))\n",
    "    col_number = df.shape[1]\n",
    "    line_plot = math.ceil(np.sqrt(col_number))\n",
    "    col_list = df.columns.values\n",
    "    for i in range(col_number):\n",
    "        plt.subplot(line_plot,line_plot,i+1) # because the first is 1 in the grid\n",
    "        sns.distplot(df[[col_list[i]]],label = df.columns[i])\n",
    "        plt.xlabel(list(df)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_stats_of_df(df, label_title =\"\", figsize = (30,20)):\n",
    "    '''Display and plot stats about the columns of the given dataframe (a boxplot, an histogram and scatter matrix)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        The dataframe\n",
    "    label_title : string\n",
    "        The label used for titles\n",
    "    figsize : (int,int)\n",
    "        The size of the figures\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    '''\n",
    "    print (\" ************   \" + label_title + \"   ************\")\n",
    "    \n",
    "    display(df.describe())\n",
    "    \n",
    "    df.boxplot(rot=90, figsize = figsize, notch = True)\n",
    "    plt.title(\"Boxplot of \" + label_title)\n",
    "    plt.show()\n",
    "    \n",
    "    df.hist(figsize = figsize)\n",
    "    plt.title(\"Histogram of \" + label_title)\n",
    "    plt.show()\n",
    "    \n",
    "    scatter_matrix(df, alpha=0.2, figsize = figsize, diagonal='kde')\n",
    "    plt.title(\"Scatter matrix of \" + label_title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time - index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Time_Series/TimeSeries.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pop.interpolate(method ='linear', limit_direction ='forward') # regarder les méthodes de fill de pandas read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(df['hour'], format = \"%y%m%d%H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop['year'] = pd.to_datetime(df_pop['year'],infer_datetime_format=True)\n",
    "df_pop.set_index('year',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['IAS_brut'].replace(0,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['IAS_brut'].interpolate(method='time', inplace=True)\n",
    "#print(sum(df['IAS_brut'].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Econometric_analysis_of_wage_repartition/blob/master/Econometric_analysis_of_wage_repartition.ipynb\n",
    "df['Inf'] = ((df.CPI-df.CPI.shift())/(df.CPI.shift()))*100\n",
    "\n",
    "import matplotlib.dates as mdates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T12:03:43.638164Z",
     "start_time": "2019-06-14T12:03:43.633178Z"
    }
   },
   "source": [
    "### Time - Shift column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IAS_brut_log_shift'] = df.IAS_brut_log.shift(365, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time - Slice date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def datesplit(originalDate):\n",
    "    originalDate = str(originalDate)\n",
    "    \n",
    "    year = int(\"20\" + originalDate[0:2])\n",
    "    month = int(originalDate[2:4])\n",
    "    day = int(originalDate[4:6])\n",
    "    hour = int(originalDate[6:8])\n",
    "    \n",
    "    return datetime.datetime(year, month, day, hour)\n",
    "\n",
    "# Exemple :\n",
    "datesplit(14102915).weekday(), datesplit(14102915).hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weekday'] = df['hour'].apply(lambda x : datesplit(x).weekday())\n",
    "df['hour'] =  df['hour'].apply(lambda x : datesplit(x).hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_date_after(df, date):\n",
    "    '''Filter the dataframe index to keep the dates after the given date\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe \n",
    "        The dataframe to filter\n",
    "    date : string\n",
    "        The date to start the new dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DF : dataframe\n",
    "        The filtered dataframe\n",
    "    '''\n",
    "    start_time = df.index[0]\n",
    "    end_time = df.index[-1]\n",
    "    datetime = pd.to_datetime([date], format = \"%Y-%m-%d\")\n",
    "    #datetime = datetime #- timedelta(days = 1)\n",
    "    \n",
    "    if datetime < start_time or datetime > end_time:\n",
    "        print(\"-- Release date \" + str(datetime)+ \" is not in the given game file: \" + df.name + \" --\")\n",
    "        return df\n",
    "    \n",
    "    df_name = df.name\n",
    "    DF = df.copy()\n",
    "    DF = DF.loc[date:]\n",
    "    DF.name = df_name\n",
    "\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_time_rows(df, days = 720):\n",
    "    '''Filter the dataframe to only keep at most \"days\" lines\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        The dataframe to filter\n",
    "    days : int\n",
    "        The maximum number of days the final dataframe has to have\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DF: dataframe\n",
    "        The dataframe limited to \"days\" days\n",
    "    '''\n",
    "    end_time = df.index[-1]\n",
    "\n",
    "    df_name = df.name\n",
    "    DF = df.copy()\n",
    "    \n",
    "    end_window = df.index[0] + timedelta(days = days - 1)\n",
    "    if end_window > end_time:\n",
    "        end_window = end_time\n",
    "    \n",
    "    DF = DF.loc[df.index[0]:end_window]\n",
    "    \n",
    "    DF.name = df_name\n",
    "    \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_window_df(df, window = {\"start\" : 0, \"end\": 720}):\n",
    "    '''Keep the index lines between the two moment given as window\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        The dataframe to get the given period of time\n",
    "    window : {\"start\":int, \"end\":int}\n",
    "        The start and end line number in a dictionary\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DF : dataframe\n",
    "        The dataframe between the two moments\n",
    "    '''\n",
    "    DF = df.copy()\n",
    "    start = window[\"start\"]\n",
    "    end = window[\"end\"]\n",
    "\n",
    "    \n",
    "    if end > df.shape[0]-1:\n",
    "        end = df.shape[0]-1\n",
    "    if start < 0:\n",
    "        start = 0\n",
    "        \n",
    "    return DF.iloc[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NA handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IAS_brut_log_shift'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop.interpolate(method ='linear', limit_direction ='forward') # regarder les méthodes de fill de pandas read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IAS_brut'].interpolate(method='time', inplace=True)\n",
    "print(sum(df['IAS_brut'].isna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IAS_brut'].replace(0,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[[0]].replace({'ham': 0, 'spam': 1}) #Spam will be 1 and ham 0\n",
    "df.Private = df.Private.map({'Yes':1, 'No':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change categ in number\n",
    "X.Private = X.Private.astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['wage'].replace(to_replace='.', value=0, inplace=True)\n",
    "df['lwage'].replace(to_replace='.', value=0, inplace=True)\n",
    "\n",
    "# Convert the field type for 'wage' & 'lwage'\n",
    "df['wage']= pd.to_numeric(df['wage'])\n",
    "df['lwage']= pd.to_numeric(df['lwage'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X = preprocessing.scale(X_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFpr, f_regression, f_classif\n",
    "# Feature selection\n",
    "sel = SelectFpr(f_regression,alpha=0.000001)\n",
    "model_sel = sel.fit(X,Y)\n",
    "X = sel.transform(X)\n",
    "X_test_input = sel.transform(X_test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T17:42:35.277838Z",
     "start_time": "2019-06-14T17:42:35.272340Z"
    }
   },
   "source": [
    "### Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Visualisation\n",
    "#########################\n",
    "\n",
    "# We compute the eigenvalues and eigenvectors\n",
    "eigenValues, eigenVectors = np.linalg.eigh(pd.DataFrame(xtrain).cov())\n",
    "#eigenValues\n",
    "\n",
    "# We sort the values\n",
    "idx = eigenValues.argsort()[::-1]\n",
    "sort_eig = eigenValues[idx]    \n",
    "sort_vect = eigenVectors[:, idx]\n",
    "\n",
    "# We plot the eigenvalues ordered\n",
    "plt.figure()\n",
    "#sorted_eig = pd.Series(eigenValues).sort_values(ascending=False).reset_index()\n",
    "sorted_eig = pd.Series(sort_eig)\n",
    "graph2 = sorted_eig.plot()\n",
    "graph2.set_title(\"Ordered eigenvalues of the correlation matrix\" )\n",
    "graph2.set_ylabel(\"Eigenvalue\" )\n",
    "\n",
    "####################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=xtrain.shape[1])\n",
    "\n",
    "# We train it on the train data\n",
    "model_pca = pca.fit(xtrain)\n",
    "\n",
    "# We transform the data acording to the model (keeping 60 significativ variables)\n",
    "xtrain = pd.DataFrame(model_pca.transform(xtrain))\n",
    "xtest = pd.DataFrame(model_pca.transform(xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.tick_params(labelrotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot - Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(df['IAS_brut_log'],bins=50,color= sns.xkcd_rgb['deep blue'])\n",
    "plt.title('IAS brut in fct of a period')\n",
    "plt.ylabel('IAS brut')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "len_list = [len(train) for train in X_train]\n",
    "plt.hist(len_list, 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df['affair'].hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Case%20study/Affairs_study/Affairs_study.ipynb\n",
    "# crosstable\n",
    "pd.crosstab(df.rate_marriage, df.affair.astype(bool)).plot(kind='bar')\n",
    "plt.title('Marriage Rating Distribution by Affair Status')\n",
    "plt.xlabel('Marriage Rating')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affair_yrs_married = pd.crosstab(df.yrs_married, df.affair.astype(bool))\n",
    "affair_yrs_married.div(affair_yrs_married.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Affair Percentage by Years Married')\n",
    "plt.xlabel('Years Married')\n",
    "plt.ylabel('Percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nice one\n",
    "\n",
    "#https://github.com/ValentinLarrieu2/Econometric_analysis_of_wage_repartition/blob/master/Econometric_analysis_of_wage_repartition.ipynb\n",
    "\n",
    "df[\"wage_log\"] = np.log(df[\"wage\"])\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2,nrows=2, figsize=(15,7))\n",
    "\n",
    "gs = axs[1,0].get_gridspec()\n",
    "axs[1,1].remove()\n",
    "axs[1,0].remove()\n",
    "\n",
    "ax1 = axs[0,0]\n",
    "ax2 = axs[0,1]\n",
    "ax3 = fig.add_subplot(gs[1,:])\n",
    "\n",
    "sns.distplot(df[\"wage\"], label='Wage', color = sns.xkcd_rgb['denim blue'], ax= ax1)\n",
    "ax1.set_title(\"Salary histogram\")\n",
    "ax1.legend()\n",
    "           \n",
    "sns.distplot(df[\"wage_log\"], label='log(wage)', color = sns.xkcd_rgb['dark green'], ax = ax2)\n",
    "ax2.set_title(\"Log salary histogram\")\n",
    "ax2.legend()\n",
    "\n",
    "sns.distplot(df[\"wage\"], label='wage', color = sns.xkcd_rgb['denim blue'], ax=ax3)\n",
    "sns.distplot(df[\"wage_log\"], label='log(wage)', color = sns.xkcd_rgb['dark green'], ax=ax3)\n",
    "ax3.set_title(\"log salary histogram\")\n",
    "ax3.set_xlim(-5,10)\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot - Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "df['IAS_brut_log'].plot()\n",
    "plt.title('IAS brut in fct of a period')\n",
    "plt.ylabel('IAS brut')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot - Subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Reinforcement_Learning/Multi_armed_bandit/Multi-armed-bandit.ipynb\n",
    "\n",
    "def show_metrics(metrics):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12, 4))\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Regret')\n",
    "    ax1.plot(range(time_horizon),metrics[0], color = 'b')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_ylim(-0.02,1.02)\n",
    "    ax2.plot(range(time_horizon),metrics[1], color = 'b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot - Bar plot & curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Graph_Mining/Graph%20Mining.ipynb\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x = np.arange(num_deg),height = degree_freq, \n",
    "        color=sns.xkcd_rgb[\"denim blue\"], alpha = 0.8,label =\"Graph distribution\")\n",
    "plt.plot(np.arange(num_deg),binomial,\"o--\",\n",
    "         color =sns.xkcd_rgb[\"dull orange\"], label = \"Binomial distribution\")\n",
    "#        color=sns.xkcd_rgb[\"purple\"], alpha = 0.8,label =\"Graph distribution\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.xticks(np.arange(num_deg))\n",
    "plt.title(\"Degree distribution on normal scale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot - Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Gaussian_Mixture_Models/GMM.ipynb\n",
    "classOplot=plt.scatter(X_class0[0],X_class0[1],s=5,color=\"violet\",marker=\"o\")\n",
    "class1plot=plt.scatter(X_class1[0],X_class1[1],s=5,color=\"green\",marker=\"o\")\n",
    "mu0plot=plt.scatter(mu1[0],mu1[1],marker=\"x\",color=\"black\",s=300)\n",
    "mu1plot=plt.scatter(mu2[0],mu2[1],marker=\"x\",color=\"red\",s=300)\n",
    "plt.title(\"Our GMM clustering\")\n",
    "\n",
    "plt.legend((classOplot, class1plot, mu0plot, mu1plot),\n",
    "           ('Class 0', 'Class 1', 'Centroid 0 center', 'Centroid 1 center'),\n",
    "           scatterpoints=1,\n",
    "           loc='lower right',\n",
    "           ncol=2,\n",
    "           fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Case%20study/Collage_Acceptance_study/Collage_acceptance_study.ipynb\n",
    "for col in df.columns:\n",
    "    plt.scatter(df[col], df.Apps,)\n",
    "    plt.title('Nb applicants VS '+ col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Nb applicants')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://maelfabien.github.io/machinelearning/HyperOpt/#the-data\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(df[df.Class == 0].Time, df[df.Class == 0].Amount, c='green', alpha=0.4, label=\"Not Fraud\")\n",
    "plt.scatter(df[df.Class == 1].Time, df[df.Class == 1].Amount, c='red', label=\"Fraud\")\n",
    "plt.title(\"Amount of the transaction over time\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Linear_Regression/Linear%20Regression%20-%20PCA%20-%20Feature%20selection%20-%20Regularisation.ipynb\n",
    "extract = random.sample(range(p),4) + [p]\n",
    "plt_df = df[extract]\n",
    "scatter_matrix(plt_df,   \n",
    "               figsize=(10, 10)) \n",
    "# scroll to see the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot - Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Case%20study/Bike_rental_study/Bike_rental_study.ipynb\n",
    "def resids(df):\n",
    "    df['resids'] = df.predicted - df.demand\n",
    "    return df        \n",
    "        \n",
    "def box_resids(df):\n",
    "    import matplotlib\n",
    "    matplotlib.use('agg')  # Set backend\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    df = resids(df)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    fig.clf()\n",
    "    ax = fig.gca()  \n",
    "    df.boxplot(column = ['resids'], by = ['hr'], ax = ax)   \n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Residuals')\n",
    "    #fig.savefig('boxes' + '.png')\n",
    "    return 'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot - Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Avazu_case_study/blob/master/Avazu_CaseStudy.ipynb\n",
    "df.groupby('site_category')['click'].mean().plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T18:25:10.625708Z",
     "start_time": "2019-06-14T18:25:10.606757Z"
    }
   },
   "source": [
    "### Plot - Discribe of vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Econometric_analysis_of_wage_repartition/blob/master/Econometric_analysis_of_wage_repartition.ipynb\n",
    "f , (ax1,ax2,ax3) = plt.subplots(3,1, figsize = (10,15))\n",
    "ax1.set_title(\"Age Distribution and statistics\")\n",
    "sns.distplot(df['age'], ax=ax1, color = sns.xkcd_rgb[\"pale red\"]);\n",
    "#sns.distplot(df_below['wage'], ax=ax3, color = sns.xkcd_rgb[\"denim blue\"], label = 'Below Median')\n",
    "plt.figtext(1, 0.82, 'Statistics :\\n'+str(df['age'].describe()), axes = ax1, fontsize = 15 )\n",
    "ax2.set_title(\"Education Distribution and statistics\")\n",
    "sns.distplot(df['educ'], ax=ax2, color = sns.xkcd_rgb[\"medium green\"] );\n",
    "plt.figtext(1, 0.49, 'Statistics :\\n'+str(df['educ'].describe()), axes = ax2, fontsize = 15 )\n",
    "ax3.set_title(\"Wage Distribution and statistics\")\n",
    "sns.distplot(df['wage'], ax=ax3, color = sns.xkcd_rgb[\"denim blue\"]);\n",
    "plt.figtext(1, 0.16, 'Statistics :\\n'+str(df['wage'].describe()), axes = ax3, fontsize = 15 )\n",
    "#\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot - Repartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Econometric_analysis_of_wage_repartition/blob/master/Econometric_analysis_of_wage_repartition.ipynb\n",
    "g = sns.jointplot(x=\"educ\", y=\"wage\", data=df, kind=\"kde\", color=\"orange\")\n",
    "g.plot_joint(plt.scatter, c=\"gray\", s=30, linewidth=1, marker=\"+\")\n",
    "g.ax_joint.collections[0].set_alpha(0)\n",
    "g.set_axis_labels('educ', 'wage')\n",
    "plt.gcf().set_size_inches(12, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T17:00:23.261778Z",
     "start_time": "2019-06-14T17:00:23.246821Z"
    }
   },
   "source": [
    "### Plot - Repartition of labels for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Binary_image_classification/blob/master/Binary_image_classification_challenge.ipynb\n",
    "import math\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "\n",
    "def plot_dataframe_variables(df,y):\n",
    "    plt.figure(figsize=(30,20))\n",
    "    col_number = df.shape[1]\n",
    "    line_plot = math.ceil(np.sqrt(col_number))\n",
    "    col_list = df.columns.values\n",
    "    for i in range(col_number):\n",
    "        plt.subplot(line_plot,line_plot,i+1) # because the first is 1 in the grid\n",
    "        for label in pd.Series(y).unique():\n",
    "            sns.distplot(xtrain[[col_list[i]]].iloc[np.where(y==label)],label = str(label))\n",
    "        plt.tight_layout()\n",
    "        plt.xlabel(list(df)[i])\n",
    "        plt.title(df.columns[i])\n",
    "        plt.legend()\n",
    "\n",
    "plot_dataframe_variables(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://github.com/ValentinLarrieu2/Econometric_analysis_of_wage_repartition/blob/master/Econometric_analysis_of_wage_repartition.ipynb\n",
    "def compute_pvalue_stat(X,y,nbVar):\n",
    "    n,k = X.shape\n",
    "    matriceGram = np.dot(np.transpose(X),X)\n",
    "    matriceGram_inv = np.linalg.inv(matriceGram)\n",
    "    beta = np.dot(np.dot(matriceGram_inv,np.transpose(X)),y)\n",
    "    u = y - np.dot(X,beta)\n",
    "    sig2 = np.dot(u.T,u)/(n-(nbVar)) #+1\n",
    "    std = np.sqrt(np.diag(sig2*matriceGram_inv))\n",
    "    t = beta/std\n",
    "    SSR = np.dot(u.T,u)\n",
    "    p_distrib = stats.t.cdf(abs(t) , n-(nbVar)) #+1\n",
    "    p_value = (1 - p_distrib) * 2\n",
    "    return beta, u, sig2, std, t, SSR, p_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,k = df.shape\n",
    "X_wage = np.c_[np.ones((n,1)), df[['city', 'educ','exper', 'nwifeinc', 'kidslt6', 'kidsge6']].values]\n",
    "y_wage = df[\"wage\"]\n",
    "\n",
    "beta_wage, u_wage, sig2_wage, std_wage, t_wage, SSR_wage, p_value_wage = compute_pvalue_stat(X_wage,y_wage,X_wage.shape[1])\n",
    "\n",
    "res_wage = pd.DataFrame(u_wage)\n",
    "plt.figure(figsize =(10,5))\n",
    "sns.distplot(res_wage, color  = sns.xkcd_rgb[\"purple\"])\n",
    "plt.title(\"Residuals of the regression of wage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pvalue_stat2(X,y):\n",
    "    n,k = X.shape\n",
    "    X = np.c_[np.ones(X.shape[0],1), X.values]\n",
    "    matriceGram = np.dot(np.transpose(X),X)\n",
    "    matriceGram_inv = np.linalg.inv(matriceGram)\n",
    "    beta = np.dot(np.dot(matriceGram_inv,np.transpose(X)),y)\n",
    "    u = y - np.dot(X,beta)\n",
    "    sig2 = np.dot(u.T,u)/(n-(k)) #+1\n",
    "    std = np.sqrt(np.diag(sig2*matriceGram_inv))\n",
    "    t = beta/std\n",
    "    SSR = np.dot(u.T,u)\n",
    "    p_distrib = stats.t.cdf(abs(t) , n-(k)) #+1\n",
    "    p_value = (1 - p_distrib) * 2\n",
    "    return beta, matriceGram, u, sig2, std, t, SSR, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta, matriceGram, u, sig2, std, t, SSR, p_value = compute_pvalue_stat(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Econometric_analysis_of_wage_repartition/blob/master/Econometric_analysis_of_wage_repartition.ipynb\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.scatterplot(x=\"motheduc\", y=\"fatheduc\", data=df)\n",
    "plt.title('Plot of the two variables fatheduc and motheduc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_moth_fath, p_value = spearmanr(df[\"motheduc\"], df[\"fatheduc\"])\n",
    "print(\"The estimated correlation between those two variables is : {:.2f}\".format(corr_moth_fath))\n",
    "print(\"The p_value for this estimated correlation is : {}\".format(p_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Comment : With a p-value so low, we can reject the hypothesis that the two variables are independent with a risk of 1%. Therefore, our intuition from the graph is confirmed and those two variables are strongly correlated. However, they are not perfectly correlated and therefore might not cause a multicolinearity issues if we include them in the model. A good way to test if those variables includes a multicolinearity issues will be to check the eigen values of the Gram matrix with or without those variables and check if including those variables creates small (close to 0) eigen values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import statsmodels.api as sm\n",
    "fig, axes = plt.subplots(1, 2, figsize=(17,7))\n",
    "fig = plot_acf(df['Inf'].values[1:],ax=axes[0], lags = 50)\n",
    "axes[0].set_title(\"Autocorrelogramme of inf\")\n",
    "fig = plot_pacf(df['Inf'].values[1:], ax=axes[1], lags = 50)\n",
    "axes[1].set_title(\"Partial autocorrelogramme of inf\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Econometric_analysis_of_wage_repartition/blob/master/Econometric_analysis_of_wage_repartition.ipynb\n",
    "n,k = df.shape\n",
    "X_wage = np.c_[np.ones((n,1)), df[['city', 'educ','exper', 'nwifeinc', 'kidslt6', 'kidsge6']].values]\n",
    "y_wage = df[\"wage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_wage, u_wage, sig2_wage, std_wage, t_wage, SSR_wage, p_value_wage = compute_pvalue_stat(X_wage,y_wage,X_wage.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_wage = pd.DataFrame(u_wage)\n",
    "plt.figure(figsize =(10,5))\n",
    "sns.distplot(res_wage, color  = sns.xkcd_rgb[\"purple\"])\n",
    "plt.title(\"Residuals of the regression of wage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Econometric_analysis_of_wage_repartition/blob/master/Econometric_analysis_of_wage_repartition.ipynb\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Probit\n",
    "\n",
    "\n",
    "\n",
    "# Loglikehood of un-restrained model \n",
    "Llik_unr = model_probit.llf\n",
    "\n",
    "\n",
    "\n",
    "y_pro = df[\"inlf\"]\n",
    "X_pro = sm.add_constant(df[[\"city\",\"educ\", \"age\", \"kidslt6\", \"kidsge6\"]])\n",
    "model_probit = Probit(y_pro,X_pro).fit()\n",
    "model_probit.summary()\n",
    "\n",
    "\n",
    "# Define the constrained model without kislt6 and kidsgt6\n",
    "X_pro_rest = sm.add_constant(df[[\"city\",\"educ\", \"age\"]])\n",
    "model_probit_rest = Probit(y_pro,X_pro_rest).fit()\n",
    "Llik_rest=model_probit_rest.llf\n",
    "\n",
    "# Calculation of the p-values via the likelihood ratio\n",
    "Ratio_Llik=2*(Llik_unr-Llik_rest)\n",
    "p_vals_Llik = 1-chi2.cdf(Ratio_Llik,2)\n",
    "print(\"The p_values for our log likehood ration test is {}\".format(p_vals_Llik))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Linear_Regression/Linear%20Regression.ipynb\n",
    "# We create function to compute the interval\n",
    "def compute_conf_interval(x_arg,quantile,model_coef,G,sigma_hat_squared):\n",
    "    Y_comp = np.dot(x_arg.T, model_coef)\n",
    "    IC_conf_cp = quantile * np.sqrt(x_arg.T.dot(np.linalg.inv(G)).dot(x_arg))*np.sqrt(sigma_hat_squared)\n",
    "    IC_conf_list = [Y_comp - IC_conf_cp, Y_comp + IC_conf_cp]\n",
    "    return IC_conf_list\n",
    "\n",
    "def compute_pred_interval(x_arg,quantile,model_coef,G,sigma_hat_squared):\n",
    "    Y_comp = np.dot(x_arg.T, model_coef)\n",
    "    IC_pred_cp = quantile * np.sqrt(1 + x_arg.T.dot(np.linalg.inv(G)).dot(x_arg))*np.sqrt(sigma_hat_squared)\n",
    "    IC_pred_list = [Y_comp - IC_pred_cp, Y_comp + IC_pred_cp]\n",
    "    return IC_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the interval for alpha = 0.1\n",
    "alpha = 0.1\n",
    "n,k = X.shape\n",
    "tstud01 = stats.t.ppf(1.0 - alpha01/2, n-k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta, gramMatrix, u, sig2, std, t, SSR, p_value = compute_pvalue_stat(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_print = np.transpose(np.array([np.ones(n),df['gnp']]))\n",
    "pred_int_low = [ compute_pred_interval(X,tstud01,beta,gramMatrix,sig2)[0] for x in X_print ]\n",
    "pred_int_high = [ compute_pred_interval(X,tstud01,beta,gramMatrix,sig2)[1] for x in X_print ]\n",
    "conf_int_low = [ compute_conf_interval(X,tstud01,beta,gramMatrix,sig2)[0] for x in X_print ]\n",
    "conf_int_high = [ compute_conf_interval(X,tstud01,beta,gramMatrix,sig2)[1] for x in X_print ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# We set our variables to print\n",
    "x = df['gnp']\n",
    "y = df['invest']\n",
    "y_vals = beta[0] + beta[1] * x\n",
    "    \n",
    "plt.plot(x, y, 'o',label=\"\")\n",
    "plt.plot(x, y_vals, '-', label='Prediction')\n",
    "plt.plot(X, conf_int_low, '--', color='yellow', label='CI')\n",
    "plt.plot(X, conf_int_high, '--', color='yellow', label='CI')\n",
    "plt.plot(X, pred_int_low, '--', color='green', label='PI')\n",
    "plt.plot(X, pred_int_high, '--', color='green', label='PI')\n",
    "#plt.plot(np.log(1000),np.log(pred_gnp_1000),'x',label=\"Y predicted\",color=\"red\")\n",
    "plt.legend()\n",
    "\n",
    "# We label\n",
    "plt.xlabel('Log(GNP)', fontsize=10)\n",
    "plt.ylabel('Log(Investment)', fontsize=10)\n",
    "\n",
    "# We reduce the space arround our points\n",
    "plt.xlim([x.min()-0.1, x.max()+0.1]) # we want our graph to have some space with the data\n",
    "plt.ylim([y.min()-0.1, y.max()+0.1])\n",
    "plt.title(\"Investment as a function of GNP logarithmically rescaled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our scikit learn model\n",
    "skl_lm = linear_model.LinearRegression(fit_intercept=True)\n",
    "skl_lm.fit(pd.DataFrame(X), pd.DataFrame(Y))\n",
    "y_pred_scikit = skl_lm.predict(np.log(GNP))\n",
    "\n",
    "# We get the values we want to compare\n",
    "B0_lm = skl_lm.intercept_[0] \n",
    "B1_lm = skl_lm.coef_[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# We set our variables to print\n",
    "x = df['gnp']\n",
    "y = df['invest']\n",
    "\n",
    "# We plot our values\n",
    "plt.plot(x, y, 'o',label=\"\")\n",
    "plt.plot(x, y_vals, '-', label='Prediction of manual model')\n",
    "plt.plot(x, skl_lm.predict(pd.DataFrame(x)), '-', label='Model prediction of Scikit learn')\n",
    "plt.plot(np.log(1000),np.log(pred_gnp_1000),'x',label=\"Y predicted manually\")\n",
    "plt.plot(np.log(1000),y_pred_scikit,'x',label=\"Y predicted by Scikit\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# We label\n",
    "plt.xlabel('Log(GNP)', fontsize=10)\n",
    "plt.ylabel('Log(Investment)', fontsize=10)\n",
    "\n",
    "# We reduce the space arround our points\n",
    "plt.xlim([x.min()-0.1, x.max()+0.1]) # we want our graph to have some space with the data\n",
    "plt.ylim([y.min()-0.1, y.max()+0.1])\n",
    "plt.title(\"Investment as a function of GNP logarithmically rescaled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of the scikit learn model\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "# We prepare our data\n",
    "XX = np.arange(df['gnp'].min()-0.5, df['gnp'].max()+0.5, 0.05)\n",
    "YY = np.arange(df['interest'].min()-0.5, df['interest'].max()+0.5, 0.05)\n",
    "xx, yy = np.meshgrid(XX, YY)\n",
    "zz = B0_lm2 + B1_lm2*xx + B2_lm2*yy\n",
    "\n",
    "# We compute the plans for PI and CI\n",
    "ZZ_IC_inf = [[compute_conf_interval2(np.array([1,x1_,x2_]), quantile_001,B_hat2_vec,G2,sigma_hat_squared2)[0] \\\n",
    "              for x1_ in XX] for x2_ in YY]\n",
    "ZZ_IC_sup = [[compute_conf_interval2(np.array([1,x1_,x2_]), quantile_001,B_hat2_vec,G2,sigma_hat_squared2)[1] \\\n",
    "              for x1_ in XX] for x2_ in YY]\n",
    "ZZ_PI_inf = [[compute_pred_interval2(np.array([1,x1_,x2_]), quantile_001,B_hat2_vec,G2,sigma_hat_squared2)[0] \\\n",
    "              for x1_ in XX] for x2_ in YY]\n",
    "ZZ_PI_sup = [[compute_pred_interval2(np.array([1,x1_,x2_]), quantile_001,B_hat2_vec,G2,sigma_hat_squared2)[1] \\\n",
    "              for x1_ in XX] for x2_ in YY]\n",
    "\n",
    "# We plot our plan\n",
    "ax.plot_wireframe(xx, yy, zz, rstride=10, cstride=10, color=\"red\", label=\"Prediction plan\")\n",
    "ax.plot_wireframe(xx, yy, np.array(ZZ_IC_inf), rstride=10, cstride=10, color=\"yellow\", label=\"CI plan\")\n",
    "ax.plot_wireframe(xx, yy, np.array(ZZ_IC_sup), rstride=10, cstride=10, color=\"yellow\", label=\"CI plan\")\n",
    "ax.plot_wireframe(xx, yy, np.array(ZZ_PI_inf), rstride=10, cstride=10, color=\"green\", label=\"PI plan\")\n",
    "ax.plot_wireframe(xx, yy, np.array(ZZ_PI_sup), rstride=10, cstride=10, color=\"green\", label=\"PI plan\")\n",
    "\n",
    "ax.plot(df['gnp'],df['interest'],Y,'o', label='Investment')\n",
    "\n",
    "# We name our axes\n",
    "plt.title(\"Investment as a function of GNP and interest\")\n",
    "plt.xlabel('log(GNP)', fontsize=10)\n",
    "plt.ylabel('Interest', fontsize=10)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Shallow_Neural_Network/SNN_Mnist.ipynb\n",
    "\n",
    "def to_one_hot(y, n_classes=10): \n",
    "    _y = np.zeros((len(y), n_classes))\n",
    "    _y[np.arange(len(y)), y] = 1\n",
    "    return _y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Recurent_Neuron_Network/IMDB_Back_Tensorflow_keras/RNN_Back.ipynb\n",
    "def ohe_note(note):\n",
    "  X_ohe = np.zeros((n_x,))\n",
    "  X_ohe[note-1] = 1\n",
    "  return X_ohe    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Avazu_case_study/blob/master/Avazu_CaseStudy.ipynb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "site_categ_label = LabelEncoder().fit(Xtrain['site_category'])\n",
    "app_categ_label = LabelEncoder().fit(Xtrain['app_category'])\n",
    "\n",
    "Xtrain['site_category'] = site_categ_label.transform(Xtrain['site_category'])\n",
    "Xtrain['app_category'] = app_categ_label.transform(Xtrain['app_category'])\n",
    "Xtest['site_category'] = site_categ_label.transform(Xtest['site_category'])\n",
    "Xtest['app_category'] = app_categ_label.transform(Xtest['app_category'])\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder().fit(Xtrain[some_columns])\n",
    "\n",
    "Xtrain_oh = ohe.transform(Xtrain[some_columns]) # fit the transformation of Xtrain restricted to selected columns\n",
    "Xtest_oh = ohe.transform(Xtest[some_columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehension list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Recurent_Neuron_Network/IMDB_Back_Tensorflow_keras/RNN_IMDB.ipynb\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T12:40:42.532124Z",
     "start_time": "2019-06-14T12:40:42.525147Z"
    }
   },
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Suport_Vector_Machine/SVM_cancer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm #datasets\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test 3 different type of models\n",
    "for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):\n",
    "    clf = svm.SVC(kernel=kernel, gamma=0.8) #we tried to ajust the gamma in order to avoid a too specific model (overfitting)\n",
    "    clf.fit(X_train, y_train) #We fit our model\n",
    "\n",
    "    #We  predict with our model what the target is :\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # We verify our predictions :\n",
    "    print(\"\\n Method \",kernel)\n",
    "    print (\"y_test = \",y_test)\n",
    "    print(\"y_predicted = \", y_pred)\n",
    "    print(\"r score =\", clf.score(X_test, y_test))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(\"r2 score : =\", r2)\n",
    "    #We print the classification matrix\n",
    "    print('Verification : \\n', metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "    # CF plot then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Avazu_case_study/blob/master/Avazu_CaseStudy.ipynb\n",
    "lr = LogisticRegression()\n",
    "lr.fit(Xtrain_oh_filtered,ytrain)\n",
    "hard = lr.predict(Xtest_oh_filtered)\n",
    "soft = lr.predict_proba(Xtest_oh_filtered)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "hard_score_feature_engin = accuracy_score(ytest, hard)\n",
    "print(\"Hard score \", hard_score_feature_engin)\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(ytest,  soft[:,1])\n",
    "plt.plot(fpr,tpr, '-')\n",
    "plt.plot([0,1],[0,1], linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "roc_feature_engin = roc_auc_score(ytest, soft[:,1])\n",
    "print(\"Area under roc curve : \", roc_feature_engin)\n",
    "\n",
    "plotlift(ytest, soft[:,1])\n",
    "plt.axvline(x= 0.2 , linestyle='--', color='r') # Fill here\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = strftime(\"%Y-%m-%d_%H-%M\", gmtime())\n",
    "np.savetxt('sub\\\\ytest_challenge_larrieu_valentin-'+time+'.csv', ytest, fmt = '%1.0d', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/CNN_with_Keras_for_image_classification/blob/master/CNN_with%20Keras_for_image_classification.ipynb\n",
    "time = strftime(\"%Y-%m-%d_%H-%M\", gmtime())\n",
    "\n",
    "# Generate the prediction\n",
    "val_pred = model.predict(test_image_challenge)\n",
    "val_pred = (val_pred > 0.5) * 1.0\n",
    "\n",
    "np.savetxt('drive/My Drive/Google Colab/Challenge_CNN/output/val_pred_challenge_larrieu_valentin-'+time+'.txt', val_pred, fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriving var name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "testooo = 12345\n",
    "\n",
    "def retrieve_name(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "\n",
    "print (retrieve_name(testooo)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class1 = np.concatenate((X_class1, X[:, [i]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dayOfWeek = X.dayOfWeek.astype(\"category\").cat.codes # convert categorical into numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearize_my_df(df):\n",
    "    '''Return the linearized version of every columns of the given dataframe ( A*X + B)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        The dataframe to linearize\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DF : dataframe\n",
    "        The dataframe with values linearized\n",
    "    '''\n",
    "    DF = copy.deepcopy(df)\n",
    "    size = len(df)\n",
    "    array_index = np.arange(size)\n",
    "    for col in df.columns:\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(array_index, DF[col].values)\n",
    "        DF[col] = slope * array_index + intercept\n",
    "    DF.index = array_index\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_non_null_df(df):\n",
    "    '''Create a \"MEAN\" column in the dataframe that computes the mean of all the other column without counting the null and zeros\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        The dataframe to count the mean of the columns from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DF : dataframe\n",
    "        The dataframe with a column \"MEAN\"\n",
    "    '''\n",
    "    DF = copy.deepcopy(df)\n",
    "    DF[\"MEAN\"] = 0\n",
    "    for index_kw in range(len(DF.index)):\n",
    "        cpt_non_null = 0\n",
    "        for index_col in range (len(DF.columns)-1):\n",
    "            value = 0\n",
    "            col_val = DF.iloc[index_kw,index_col]\n",
    "            if not(col_val == 0 or np.isnan(col_val)):\n",
    "                value = col_val\n",
    "                DF.iloc[index_kw,-1] = DF.iloc[index_kw,-1] + value\n",
    "                cpt_non_null = cpt_non_null + 1\n",
    "        DF.iloc[index_kw,-1] = DF.iloc[index_kw,-1] / cpt_non_null\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_times_col(df):\n",
    "    '''Add columns related to time to the given dataframe:\n",
    "    - \"FIRST_CHRISTMAS\" is equal to one between the 24 and the 31 of the first december in the dataframe\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        The dataframe to modify\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : dataframe\n",
    "        The dataframe with the time related column added\n",
    "    '''\n",
    "    DF = copy.deepcopy(df)\n",
    "    #df['DAY_OF_WEEK'] = [df.index[i].dayofweek for i in range(df.shape[0])]\n",
    "    #df['QUARTER'] = [df.index[i].quarter for i in range(df.shape[0])]\n",
    "    #df['CHRISTMAS'] = [1 if ((x.day >= 25) and (x.month == 12)) else 0 for x in df.index]\n",
    "    DF['FIRST_CHRISTMAS'] = 0\n",
    "    DF['SECOND_CHRISTMAS'] = 0\n",
    "    first_christmas = False\n",
    "    second_christmas = False\n",
    "    \"\"\"while not first_christmas:\n",
    "        for ind in DF.index:\n",
    "            if ind.month == 12 and ind.day >= 24 :\n",
    "                DF.loc[ind,'FIRST_CHRISTMAS'] = 1\n",
    "            if ind.month == 12 and ind.day == 31 :\n",
    "                first_christmas = True\n",
    "                break\n",
    "        first_christmas = True\"\"\"\n",
    "    while not (first_christmas and second_christmas):\n",
    "        for ind in DF.index:\n",
    "            if first_christmas and ind.month == 12 and ind.day >= 24 :\n",
    "                DF.loc[ind,'SECOND_CHRISTMAS'] = 1\n",
    "            if first_christmas and ind.month == 12 and ind.day == 31 :\n",
    "                second_christmas = True    \n",
    "                break\n",
    "            if ind.month == 12 and ind.day >= 24 :\n",
    "                DF.loc[ind,'FIRST_CHRISTMAS'] = 1\n",
    "            if ind.month == 12 and ind.day == 31 :\n",
    "                first_christmas = True\n",
    "        first_christmas = True\n",
    "        second_christmas = True\n",
    "        \n",
    "\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_power_df(df, power, columns = None):\n",
    "    '''Creates a dataframe corresponding to the columns of \"df\" to the power \"power\"\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        The dataframe to modify\n",
    "    power : int\n",
    "        The power to apply to each columns\n",
    "    columns : list[String]\n",
    "        The list of folumn to apply power function on. By default if not specified it will be applied on all the columns\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_res : dataframe\n",
    "        The dataframe of the new columns corresponding to the original columns to the power \"power\"\n",
    "    '''\n",
    "    DF = copy.deepcopy(df)\n",
    "    df_res = pd.DataFrame()\n",
    "    \n",
    "    if not columns:\n",
    "        col_application = list(DF.columns)\n",
    "    else:\n",
    "        col_application = columns\n",
    "        \n",
    "    for col in col_application:\n",
    "        if col == \"USERS_ACQUIRED\":\n",
    "            continue\n",
    "        val = DF[col]\n",
    "        val = val.pow(power)    \n",
    "        \n",
    "        #val = val.fillna(0)\n",
    "        #val = np.nan_to_num(val)\n",
    "        val = val.replace([np.inf, -np.inf], np.nan)\n",
    "        val = val.fillna(val.mean)\n",
    "        df_res[col + \"_POWER_\" + str(power)] = val\n",
    "        #DF[col + \"_POWER_\" + str(power)] = val\n",
    "        #DF[col + \"_POWER_\" + str(power)] = DF[col + \"_POWER_\" + str(power)].fillna(DF[col + \"_POWER_\" + str(power)].mean())\n",
    "        \"\"\"plt.plot(val)\n",
    "        plt.show()\"\"\"\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_combination_of_columns(df, nb_combination_max = None, nb_combination_min = None, columns = None):\n",
    "    '''Adds columns to the dataframe. They are the combinations of the one given originaly (sum and multiplication). \n",
    "    By default all combinations will be tested (nb_col -1 combinations until 2) but it is possible to specify the number\n",
    "    min and max of combination to test\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        The dataframe to add the combination of columns to\n",
    "    nb_combination_max : int\n",
    "        The number of combination of column max we want to try (example: if 4, we will try all combination of 4 columns)\n",
    "    nb_combination_min : int\n",
    "        The number of combination of column min we want to try (example: if 2, we will try all combination of 2 columns)\n",
    "    columns : list[String]\n",
    "        The list of folumn to apply power function on. By default if not specified it will be applied on all the columns\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DF : dataframe\n",
    "        The dataframe with the new combinations column added\n",
    "    '''\n",
    "    \n",
    "    DF = copy.deepcopy(df)\n",
    "    \n",
    "    if not columns:\n",
    "        original_col_list = list(DF.columns)\n",
    "    else:\n",
    "        original_col_list = columns\n",
    "    \n",
    "    nb_original_col = len(original_col_list)\n",
    "    dict_all_combinations = {}\n",
    "    \n",
    "    if not (nb_combination_max and nb_combination_min):\n",
    "        nb_combination_max = nb_original_col-1\n",
    "        nb_combination_min = 2\n",
    "\n",
    "    # Creation of the combinations\n",
    "    for nb_combi in range (nb_combination_max, nb_combination_min-1, -1):\n",
    "        list_combinations = list(it.combinations(original_col_list, nb_combi))\n",
    "        dict_all_combinations[nb_combi] = list_combinations\n",
    "\n",
    "\n",
    "    for combination_level in dict_all_combinations.keys(): # For each type of combination (example: two by two, three ny three)\n",
    "        for combination in dict_all_combinations[combination_level]: # For each combination of columns\n",
    "            name_col = \"\"\n",
    "\n",
    "            df_sum = pd.DataFrame()\n",
    "            df_mult = pd.DataFrame()\n",
    "            df_sum[\"col\"] = [0 for i in range(len(DF))]\n",
    "            df_mult[\"col\"] = [1 for i in range(len(DF))]\n",
    "            df_sum.index = DF.index\n",
    "            df_mult.index = DF.index\n",
    "\n",
    "            for column_name in combination:\n",
    "                df_sum[\"col\"] = df_sum[\"col\"] + DF[column_name]\n",
    "                df_mult[\"col\"] = df_mult[\"col\"] * DF[column_name]\n",
    "\n",
    "\n",
    "                #if column_name == \"\": # We change the empty keyword to be easy to see as a column name\n",
    "                #    column_name = \"empty\"\n",
    "                name_col = name_col + \"_\" + column_name\n",
    "            name_col = name_col[1:] # We remove the starting \"_\"\n",
    "\n",
    "            df_sum = df_sum.rename(columns = {\"col\" : \"SUM_\" + name_col})\n",
    "            df_mult = df_mult.rename(columns = {\"col\" : \"MULT_\" + name_col})\n",
    "            df_mult = df_mult\n",
    "\n",
    "            DF = pd.concat([DF, df_sum], axis=1)\n",
    "            DF = pd.concat([DF, df_mult**(1/combination_level)], axis=1)\n",
    "            #DF = pd.concat([DF, df_mult], axis=1)\n",
    "            \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df, columns = None):\n",
    "    '''Adds columns to the dataframe, some being all the columns but retrieved from their mean (after 21 days here), some being peak detected\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe \n",
    "        The dataframe\n",
    "    columns : list[String]\n",
    "        The list of folumn to apply power function on. By default if not specified it will be applied on all the columns\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DF : dataframe\n",
    "        The dataframe with the features added\n",
    "    '''\n",
    "    \n",
    "    DF = copy.deepcopy(df)\n",
    "\n",
    "    if not columns:\n",
    "        original_col_list = list(DF.columns)\n",
    "    else:\n",
    "        original_col_list = columns\n",
    "\n",
    "    # Combination\n",
    "    DF = add_combination_of_columns(DF, columns = original_col_list)\n",
    "\n",
    "\n",
    "    # Power\n",
    "    DF2 = create_power_df(DF, 2, columns = original_col_list)\n",
    "    DF = pd.concat([DF, DF2], axis = 1)\n",
    "\n",
    "    \n",
    "    return DF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T19:30:41.822533Z",
     "start_time": "2019-06-14T19:30:41.780647Z"
    }
   },
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:31:13.264995Z",
     "start_time": "2019-06-14T15:31:13.256017Z"
    }
   },
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use cross validation\n",
    "from sklearn import cross_validation\n",
    "\n",
    "clf = LogisticRegression(random_state=1)\n",
    "scores = cross_validation.cross_val_score(clf, X, y, scoring='accuracy', cv=5) #scoring='r2'\n",
    "\n",
    "print ('scores mean =', scores.mean(), 'scores std =', scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Case%20study/Bike_rental_study/Bike_rental_study.ipynb\n",
    "\n",
    "from sklearn.model_selection  import GridSearchCV\n",
    "params = {'min_samples_leaf':list([2, 4, 6]),'min_samples_split':list([2, 5, 7, 9, 11]),\n",
    "          'n_estimators':list([30,40,50])}\n",
    "clf_rf4=RandomForestRegressor(random_state=1)\n",
    "# We want to find the best parameters\n",
    "clf_gs3=GridSearchCV(clf_rf4, params, scoring = 'r2',cv=5)\n",
    "clf_gs3.fit(df[predictors], df[\"demand\"])\n",
    "\n",
    "\n",
    "\n",
    "print(clf_gs3.best_score_)\n",
    "print(clf_gs3.best_params_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T00:10:11.619525Z",
     "start_time": "2020-12-16T00:10:11.606978Z"
    }
   },
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import tpe\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin\n",
    "from hyperopt import STATUS_OK\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'boosting_type': hp.choice('boosting_type', \n",
    "                               [{'boosting_type': 'gbdt', \n",
    "                                    'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "                                 {'boosting_type': 'goss'}]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 10, 400, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 1000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 1),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.001), np.log(1.0)),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', np.log(0.001), np.log(1.0)),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n",
    "    'n_jobs':-1,\n",
    "    'n_estimators': 500,\n",
    "    'seed':SEED\n",
    "}\n",
    "\n",
    "space = {\n",
    "'min_samples_split': 2 + hp.randint('min_samples_split', 49),\n",
    "'min_samples_leaf': 1 + hp.randint('min_samples_leaf', 49),\n",
    "'max_depth': 5 + hp.randint('max_depth', 25),\n",
    "'criterion': hp.choice('criterion',{'mse', 'mae'}),\n",
    "'n_jobs': -1,\n",
    "'n_estimators': 100 + hp.randint('n_estimators', 700)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_my_model(model_given, space, X_train, y_train, X_test, y_test):\n",
    "    '''Compute the comparison dataframe between the baseline and y\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_given : model\n",
    "        The model to optimize\n",
    "    space\n",
    "        The parameter space to optimize\n",
    "    X_train : dataframe\n",
    "        The training X data\n",
    "    y_train : dataframe\n",
    "        The training y data\n",
    "    X_test : dataframe\n",
    "        The test X data\n",
    "    y_test : dataframe\n",
    "        The test y data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    params \n",
    "        The parameters optimized to use to get the best score in that model\n",
    "    '''\n",
    "    trials = Trials()\n",
    "\n",
    "    # Algorithm\n",
    "    tpe_algorithm = tpe.suggest\n",
    "\n",
    "    def objective(params):\n",
    "        \"\"\"Objective function for Gradient Boosting Hyperparameter Tuning\"\"\"\n",
    "\n",
    "        #Our XGBRegressor\n",
    "        model = model_given\n",
    "\n",
    "        #We set the parameters of the model\n",
    "        model.set_params(**params)\n",
    "\n",
    "        #We calculate the loss with our cv function\n",
    "        loss = compute_loss_RMSE(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "        #We return a dictionnary for hyperopt framework\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "    # Optimize\n",
    "    t1=time.time()\n",
    "\n",
    "    optimized = fmin(fn = objective,\n",
    "                     space = space,\n",
    "                     algo = tpe.suggest,\n",
    "                     max_evals = 100,\n",
    "                     trials = trials)\n",
    "\n",
    "    t2 = time.time()\n",
    "    \n",
    "    \n",
    "    params = trials.best_trial['result']['params']\n",
    "    return (params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T13:39:12.188746Z",
     "start_time": "2019-06-14T13:39:12.174813Z"
    }
   },
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Naive_Bayes/mail_classification.ipynb\n",
    "def make_Dictionary(X):\n",
    "    all_words = []\n",
    "    for line in X:\n",
    "        words = line.split()\n",
    "        all_words += words\n",
    "\n",
    "    dictionary = Counter(all_words)\n",
    "    # list_to_remove = dictionary.keys()\n",
    "    # for item in list_to_remove: # this works with python 2.x version\n",
    "    for item in list(dictionary): # this works with python 3.x version\n",
    "        if item.isalpha() == False:\n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    return dictionary\n",
    "\n",
    "def extract_features(X):\n",
    "\n",
    "    features_matrix = np.zeros((len(X), 3000))\n",
    "    docID = 0\n",
    "    for line in X:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            wordID = 0\n",
    "            for i, d in enumerate(dictionary):\n",
    "                if d[0] == word:\n",
    "                    wordID = i\n",
    "                    features_matrix[docID, wordID] = words.count(word)\n",
    "        docID = docID + 1\n",
    "    return features_matrix\n",
    "\n",
    "# Create a dictionary of words with its frequency\n",
    "dictionary = make_Dictionary(X_train)\n",
    "\n",
    "\n",
    "# Prepare feature vectors per training mail and its labels\n",
    "train_matrix = extract_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Independent_Component_Analysis_and_Nonnegative_Matrix_Factorization/ICA%20NMF.ipynb\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 900\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it using the most common word\n",
    "# frequency with TF-IDF weighting (without top 5% stop words)\n",
    "\n",
    "t0 = time()\n",
    "print(\"Loading dataset and extracting TF-IDF features...\")\n",
    "dataset = datasets.fetch_20newsgroups(data_home='c:/tmp/', shuffle=True,\n",
    "                                      random_state=1)\n",
    "\n",
    "vectorizer = text.CountVectorizer(max_df=0.95, max_features=n_features)\n",
    "counts = vectorizer.fit_transform(dataset.data[:n_samples])\n",
    "tfidf = text.TfidfTransformer().fit_transform(counts)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Fit the NMF model\n",
    "# print(\"Fitting the NMF model on with n_samples=%d and\" +\n",
    "#       \"n_features=%d...\" % (n_samples, n_features))\n",
    "nmf = decomposition.NMF(n_components=n_topics).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Inverse the vectorizer vocabulary to be able\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Graph_Mining/Graph%20Mining.ipynb\n",
    "# Let's generarate score and labels (true or false edges) with jaccard\n",
    "pred_jaccard = list(nx.jaccard_coefficient(G_fb_train))\n",
    "score_jaccard, label_jaccard = zip(*[(s, (u,v) in edge_subset) for (u,v,s) in pred_jaccard])\n",
    "fpr_jaccard, tpr_jaccard, _ = metrics.roc_curve(label_jaccard, score_jaccard)\n",
    "auc_jaccard = metrics.auc(fpr_jaccard, tpr_jaccard)\n",
    "\n",
    "pred_adamic = list(nx.adamic_adar_index(G_fb_train))\n",
    "score_adamic, label_adamic = zip(*[(s, (u,v) in edge_subset) for (u,v,s) in pred_adamic])\n",
    "fpr_adamic, tpr_adamic, _ = metrics.roc_curve(label_adamic, score_adamic)\n",
    "auc_adamic = metrics.auc(fpr_adamic, tpr_adamic)\n",
    "\n",
    "pred_pref = list(nx.preferential_attachment(G_fb_train))\n",
    "score_pref, label_pref = zip(*[(s, (u,v) in edge_subset) for (u,v,s) in pred_pref])\n",
    "fpr_pref, tpr_pref, _ = metrics.roc_curve(label_pref, score_pref)\n",
    "auc_pref = metrics.auc(fpr_pref, tpr_pref)\n",
    "\n",
    "# Plot ROC curve and AUC\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(fpr_jaccard, tpr_jaccard, label='Jaccard Coefficient - AUC %.2f' % auc_jaccard,\n",
    "        alpha = 0.8)\n",
    "plt.plot(fpr_adamic, tpr_adamic, label='Adamic-Adar Index - AUC %.2f' % auc_adamic, \n",
    "         alpha = 0.8)\n",
    "plt.plot(fpr_pref, tpr_pref, label='Preferential Attachments - AUC %.2f' % auc_pref, \n",
    "         alpha = 0.8)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(\"ROC Curve for different similarities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Decision_Tree/Decision_tree.ipynb\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "\n",
    "title = \"Learning Curves (Naive Bayes)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = tree.DecisionTreeClassifier(max_depth=np.argmin(grid_score_q7[0]))\n",
    "plot_learning_curve(estimator, title, X, y, ylim=(0.1, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Case%20study/Cover_Type_challenge/sklearn_ExtraTrees_model.ipynb\n",
    "\n",
    "# We fit our model\n",
    "et = AdaBoostClassifier(ExtraTreesClassifier(n_estimators=300, criterion= 'entropy', n_jobs = -1, warm_start=True, max_features = 20), n_estimators=1000, learning_rate=0.0001, algorithm='SAMME.R')\n",
    "\n",
    "et.fit(X_train,Y_train)\n",
    "\n",
    "# We use it to predict our output\n",
    "Y_hat = et.predict(X_test)\n",
    "\n",
    "# We print the results\n",
    "print(metrics.classification_report(Y_test,Y_hat))\n",
    "print(\"ExtraTrees Accuracy :\", metrics.accuracy_score(Y_test,Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost list of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "dept_list = [l for l in range (1,11)]\n",
    "nb_model = 0\n",
    "rf_list = []\n",
    "dt_list = []\n",
    "rf_ada_list = []\n",
    "dt_ada_list = []\n",
    "model_list = []\n",
    "score_list_of_list = []\n",
    "for dept in dept_list: \n",
    "    score_list = []\n",
    "    model_list = [RandomForestClassifier(max_depth=dept), DecisionTreeClassifier(max_depth=dept)]\n",
    "    model_list_ada = []\n",
    "    # We adaboost the model\n",
    "    for model in model_list:\n",
    "        model_ada = AdaBoostClassifier(base_estimator = model)\n",
    "        model_list_ada.append(model_ada)\n",
    "    model_list = model_list + model_list_ada\n",
    "    nb_model = len(model_list)\n",
    "    \n",
    "    for model in model_list:\n",
    "        cross_score = cross_val_score(model, X, y, cv=6)\n",
    "        score_list.append(np.mean(cross_score))\n",
    "    score_list_of_list.append(score_list) #list of list : each list is perf for model 1 to 15 on this dept\n",
    "\n",
    "    #.set_params( max_depth = dept)\n",
    "    \n",
    "np_list = np.array(score_list_of_list)\n",
    "##\n",
    "import inspect\n",
    "\n",
    "def retrieve_name(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "##\n",
    "\n",
    "plt.figure()\n",
    "for i in range(nb_model):\n",
    "    plt.plot(dept_list,np_list[:,i],label=\"model\"+str(i))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIM CLASSIFICATION\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Loading the Digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.8, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "\n",
    "def optimise_my_model(classifier, tuned_parameters, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    scores = ['precision', 'recall', 'f1'] # macro not for f1\n",
    "    ## CF https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "    for score in scores:\n",
    "        print(\"################################################################\")\n",
    "        print(\"\\nTuning hyper-parameters for \\n ###### %s ###### \\n\" % score)\n",
    "        print()\n",
    "\n",
    "        clf = GridSearchCV(classifier, tuned_parameters, cv=5,\n",
    "                           scoring='%s_macro' % score)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"#########\")\n",
    "        print(\"Best parameters set found on development set:\\n\")\n",
    "        print(clf.best_params_)\n",
    "        print(\"#########\\n\")\n",
    "        print(\"Grid scores on development set:\\n\")\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "\n",
    "        print(\"\\nDetailed classification report:\\n\")\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\\n\")\n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    \n",
    "optimise_my_model(SVC(), tuned_parameters, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGBM\n",
    "clf = LGBMClassifier(verbose=2, learning_rate=0.1, n_estimators=100, random_state = SEED, n_jobs=-1,)\n",
    "\n",
    "tuned_parameters_xg = [{'random_state': [SEED], 'verbose': [1], 'n_jobs': [-1],  'n_estimators': [100,200], \\\n",
    "                       'learning_rate': [0.1,0.3,0.4]}]\n",
    "optimise_my_model(clf, tuned_parameters_xg, xtrain_split, xtest_split, ytrain_split, ytest_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt - Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T23:45:04.517077Z",
     "start_time": "2020-12-15T23:45:04.503331Z"
    }
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://maelfabien.github.io/machinelearning/HyperOpt/#hyperopt\n",
    "\n",
    "from hyperopt import tpe\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import Trials\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin\n",
    "\n",
    "N_FOLDS = 10\n",
    "MAX_EVALS = 50\n",
    "\n",
    "def objective(params, n_folds = N_FOLDS):\n",
    "    \"\"\"Objective function for Logistic Regression Hyperparameter Tuning\"\"\"\n",
    "\n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "\n",
    "    clf = LogisticRegression(**params,random_state=0,verbose =0)\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='f1_macro')\n",
    "\n",
    "    # Extract the best score\n",
    "    best_score = max(scores)\n",
    "\n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "space = {\n",
    "    'class_weight': hp.choice('class_weight', [None, class_weight]),\n",
    "    'warm_start' : hp.choice('warm_start', [True, False]),\n",
    "    'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n",
    "    'tol' : hp.uniform('tol', 0.00001, 0.0001),\n",
    "    'C' : hp.uniform('C', 0.05, 3),\n",
    "    'solver' : hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "    'max_iter' : hp.choice('max_iter', range(5,1000))\n",
    "}\n",
    "\n",
    "# Algorithm\n",
    "tpe_algorithm = tpe.suggest\n",
    "\n",
    "# Trials object to track progress\n",
    "bayes_trials = Trials()\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials)\n",
    "\n",
    "#https://github.com/hyperopt/hyperopt/wiki/FMin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model_list = []\n",
    "clf1 = RandomForestClassifier(n_jobs = -1, n_estimators = 200, verbose=1, random_state=SEED,criterion= 'gini', max_features= 20)\n",
    "clf2 = XGBClassifier(verbose=1, learning_rate=0.4, n_estimators=200, random_state = SEED, n_jobs=-1, max_depth= 5)\n",
    "clf3 = LGBMClassifier(verbose=1, learning_rate=0.1, n_estimators=200, random_state = SEED, n_jobs=-1, min_child_weight=0.4,class_weight=None,importance_type='split',colsample_bytree=1.0,min_child_samples=20, min_split_gain=0.0,num_leaves=1000,reg_alpha=0.0,reg_lambda=0.0,subsample=1.0,subsample_for_bin=2000000,subsample_freq=0)\n",
    "clf4 = ExtraTreesClassifier(verbose=1, n_estimators=200, random_state = SEED, n_jobs=-1,max_features=30)\n",
    "clf5 = LinearDiscriminantAnalysis()\n",
    "model_list.append(clf1)\n",
    "model_list.append(clf2)\n",
    "model_list.append(clf3)\n",
    "model_list.append(clf4)\n",
    "model_list.append(clf5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_list_predictions(model_list,xtrain,ytrain,xtest):\n",
    "    for model in model_list:\n",
    "        model.fit(xtrain,ytrain)\n",
    "        prediction_train = model.predict(xtrain)\n",
    "        prediction_test = model.predict(xtest)\n",
    "        pd.concat([xtrain,pd.DataFrame(prediction_train)],axis=1)\n",
    "        pd.concat([xtest,pd.DataFrame(prediction_test)],axis=1)\n",
    "        model_list_predictions(model_list,xtrain_split,ytrain_split,xtest_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clf = LGBMClassifier(verbose=4, learning_rate=0.05, n_estimators=5000, random_state = SEED, n_jobs=-1,min_child_weight=0.4,class_weight=None,importance_type='split',colsample_bytree=1.0,min_child_samples=20, min_split_gain=0.0,num_leaves=400,reg_alpha=0.0,reg_lambda=0.0,subsample=1.0,subsample_for_bin=2000000,subsample_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clf.fit(xtrain_split, ytrain_split)\n",
    "ypred = new_clf.predict(xtest_split)\n",
    "(ypred == ytest_split).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "Xtrain_ha = pd.DataFrame()\n",
    "Xtest_ha = pd.DataFrame()\n",
    "for col in Xtrain_mg.columns:\n",
    "    Xtrain_ha[col] = Xtrain_mg[col].apply(lambda x : hash(x)%1000000) # Fill here defining a lambda function\n",
    "    Xtest_ha[col] = Xtest_mg[col].apply(lambda x : hash(x)%1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ValentinLarrieu2/Machine_Learning/blob/master/Case%20study/Cover_Type_challenge/mllib_on_spark.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canvas Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global plan of a classic case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reading of the data\n",
    "* Data quality check (integrity, repartition, plot)\n",
    "* Data cleaning (missing values, uniformisation)\n",
    "* Data transformation ( standardisation, normalisation, feature selection, feature augmentation)\n",
    "* Split of the data\n",
    "* Model training (crossvalidation, loo)\n",
    "* Model testing (F& score, recall, accuracy, R2, Loss)\n",
    "* Save (Model trained, weights, parameters, results, scores)\n",
    "* Result & Score evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some question to address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Modeling\n",
    "* Normalisation choice\n",
    "* Metric choice\n",
    "* ML algorithm choice\n",
    "\n",
    "Industrialisation\n",
    "* Parralelissation\n",
    "* Genericity\n",
    "* Packaging\n",
    "* Piping\n",
    "* Versioning\n",
    "* Comments\n",
    "* Tests\n",
    "* Optimisation\n",
    "* Monitoring\n",
    "* Compatibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# notebook\n",
    "\n",
    "# text formatting\n",
    "import re\n",
    "# slugify?\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from statistics import mean\n",
    "import json\n",
    "import time\n",
    "import unittest\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "# sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "# Vizualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.feature_selection import SelectFpr, f_regression, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1234\n",
    "website_prefix = \"https://www.uuu.com\"\n",
    "file_path = \"C:\\\\Users\\\\Orion\\\\music.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is usefull function, put it there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Harvesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ########## GET DATA FROM SOURCES\n",
    "    \n",
    "    # Scrapp\n",
    "    \n",
    "    # Api\n",
    "    \n",
    "    # Import\n",
    "    \n",
    "    #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrap the website *** to give a usefull dataframe\n",
    "\n",
    "def get_scrap_dataframe():\n",
    "\n",
    "    # TBD\n",
    "    \"\"\"\n",
    "    # CODE EXAMPLE\n",
    "    \n",
    "    url = website_prefix + \"\" #\n",
    "    soup = _handle_request_result_and_build_soup(url)\n",
    "\n",
    "    specific_class = \"search-table-data\"\n",
    "    tableContent =soup.find(class_=specific_class).find_all(\"tr\")\n",
    "    mylinks =[]\n",
    "    for ind in range (1,len(tableContent)):\n",
    "        mylinks.append(_clear_url(tableContent[ind].attrs['onclick']))\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the *** api to get usefull dataframe\n",
    "\n",
    "def get_api_dataframe():\n",
    "\n",
    "    # TBD\n",
    "    \"\"\"\n",
    "    # CODE EXAMPLE\n",
    "    \n",
    "    # We read the token which is stored in a local file\n",
    "    token = open(\"PATH\\\\gitToken.txt\", encoding=\"utf8\").readline()[:-1]\n",
    "    \n",
    "    request = requests.get(\"https://api.github.com/users/\" + username + \"/repos\", auth=(username_account, token))\n",
    "    #print(\"request code \", request.status_code)\n",
    "    \n",
    "    json_res = json.loads(request.text)\n",
    "    #print(\"jsonres\", json_res)\n",
    "    \n",
    "    try:\n",
    "        star_list = [json1[\"stargazers_count\"] for json1 in json_res]\n",
    "        dict[username] = mean(star_list)\n",
    "    except:\n",
    "        dict[username] = 0\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from a csv file\n",
    "\n",
    "\n",
    "def get_import_dataframe():\n",
    "    \n",
    "    na_values = ['?', '']\n",
    "    df = pd.read_csv(file_path, sep=',', na_values=na_values) # add chunksize=1000 to limit the size then df = next(df)\n",
    "\n",
    "    # use the global variable file_path to define the path\n",
    "    #df = pd.DataFrame()\n",
    "\n",
    "    return df\n",
    "\n",
    "#df_test = get_import_dataframe().sort_values(by='artist.hotttnesss', ascending=False, na_position='last')\n",
    "#df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the data of the 3 dataframe\n",
    "\n",
    "def get_joined_dataframe(df_scrap, df_api, df_import):\n",
    "    \n",
    "    # TBD\n",
    "    \n",
    "    \"\"\"\n",
    "    # CODE EXAMPLE\n",
    "    \n",
    "    joined_df_with_pop = pd.merge(joined_df, population_filtered, on='DEPARTEMENTID', left_index=True, right_index=False)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframe\n",
    "\n",
    "def get_cleaned_dataframe(df_in):\n",
    "    \n",
    "    # TBD\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    return df  \n",
    "\n",
    "def get_cleaned_import_dataframe(df_in):\n",
    "    \n",
    "    # TBD\n",
    "    df = df_in.drop(columns=['artist.id', 'artist.name', 'artist_mbtags', 'artist.name', 'location', 'release.id', 'release.name','similar','song.id', 'terms', 'title'])\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an interesting thing\n",
    "\n",
    "def vizualise_interesting_thing_1(df_in):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # CODE EXAMPLE\n",
    "    \n",
    "    plot_pop_depassement = df_in.plot(x='POPULATION', y='POURCENTAGE_DEPASSEMENT', marker='o', color='red', title = \"POPULATION as a function of POURCENTAGE_DEPASSEMENT\")\n",
    "    plt.show()\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(88)\n",
    "    plot_pop_depassement = df_in.plot(x='familiarity', y='song.hotttnesss', linewidth = 0, marker='x', color='red', title = \"Familarity as a function of song hotness\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(88)\n",
    "    plot_pop_depassement = df_in.plot(x='familiarity', y='artist.hotttnesss', linewidth = 0, marker='x', color='red', title = \"Familarity as a function of artist hotness\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(88)\n",
    "    plot_pop_depassement = df_in.plot(x='longitude', y='song.hotttnesss', linewidth = 0, marker='x', color='red', title = \"Longitude as a function of song hotness\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(88)\n",
    "    plot_pop_depassement = df_in.plot(x='latitude', y='song.hotttnesss', linewidth = 0, marker='x', color='red', title = \"Latitude as a function of song hotness\")\n",
    "    plt.show()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def visualise_scatter(df_in):\n",
    "    \n",
    "    extract = [k for k in range (0,5)]\n",
    "    plt_df = df_in.iloc[:,extract]\n",
    "    scatter_matrix(plt_df,   \n",
    "                   figsize=(10, 10)) \n",
    "    \n",
    "    extract = [k for k in range (5,10)]\n",
    "    plt_df = df_in.iloc[:,extract]\n",
    "    scatter_matrix(plt_df,   \n",
    "                   figsize=(10, 10)) \n",
    "    \n",
    "    extract = [k for k in range (10,15)]\n",
    "    plt_df = df_in.iloc[:,extract]\n",
    "    scatter_matrix(plt_df,   \n",
    "                   figsize=(10, 10)) \n",
    "    \n",
    "    extract = [k for k in range (15,20)]\n",
    "    plt_df = df_in.iloc[:,extract]\n",
    "    scatter_matrix(plt_df,   \n",
    "                   figsize=(10, 10)) \n",
    "    \n",
    "    extract = [k for k in range (20,25)]\n",
    "    plt_df = df_in.iloc[:,extract]\n",
    "    scatter_matrix(plt_df,   \n",
    "                   figsize=(10, 10)) \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of the model\n",
    "\n",
    "def plot_my_model(model):\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Explotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model on the data\n",
    "\n",
    "def build_a_model(df_in):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # CODE EXAMPLE\n",
    "    \n",
    "    X = spe[\"EFFECTIFS/POPULATION\"]\n",
    "    Y = spe[\"POURCENTAGE_DEPASSEMENT\"]\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=SEED)\n",
    "    skl_lm = linear_model.LinearRegression(fit_intercept=True)\n",
    "    skl_lm.fit(X_train, Y_train)\n",
    "    score = skl_lm.score(X_test, Y_test)\n",
    "    print(\"Score: \\n\", score)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    Y = df_in[\"song.hotttnesss\"]\n",
    "    X = df_in.drop(columns=['song.hotttnesss'])\n",
    "    #X = df_in.drop(columns=['familarity'])\n",
    "    #X = df_in[[\"tempo\",\"mode\",\"loudness\"]]\n",
    "    \n",
    "    \"\"\"\n",
    "    # Normalisation of the data\n",
    "    sc = StandardScaler()\n",
    "    model_centered = sc.fit(X)\n",
    "    X = model_centered.transform(X)\n",
    "\n",
    "\n",
    "    # Feature selection\n",
    "    sel = SelectFpr(f_regression,alpha=0.1)#alpha=0.00001\n",
    "    model_sel = sel.fit(X,Y)\n",
    "    X = sel.transform(X)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=SEED)\n",
    "    #skl_m = linear_model.LinearRegression(fit_intercept=True) #0.17\n",
    "    skl_m = RandomForestRegressor(n_estimators =100) #0.23 max_depth \n",
    "    #skl_m = tree.DecisionTreeRegressor()\n",
    "    skl_m.fit(X_train, Y_train)\n",
    "    score = skl_m.score(X_test, Y_test)\n",
    "    print(\"Score: \\n\", score)\n",
    "    #feat = skl_m.feature_importance_\n",
    "    #print(\"feat\", feat)\n",
    "    \n",
    "   # plt.bar(range(len(skl_m.feature_importances_)), skl_m.feature_importances_)\n",
    "    plt.bar(X.columns, skl_m.feature_importances_)\n",
    "    plt.xticks(range(len(skl_m.feature_importances_)), X.columns, rotation='vertical')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    #print(\"aa\",X.columns)\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests definition and main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HackathonTests(unittest.TestCase):\n",
    "\n",
    "    def test_dummy(self):\n",
    "        self.assertEqual('foo'.upper(), 'FOO')\n",
    "        \n",
    "    def test_return_type(self):\n",
    "        self.assertEqual(type(get_scrap_dataframe()), type(pd.DataFrame()))    \n",
    "        \"\"\"self.assertEqual(type(get_api_dataframe()), type(pd.DataFrame()))\n",
    "        self.assertEqual(type(get_import_dataframe()), type(pd.DataFrame()))\n",
    "        self.assertEqual(type(get_joined_dataframe(pd.DataFrame(),pd.DataFrame(),pd.DataFrame())), type(pd.DataFrame()))\n",
    "        self.assertEqual(type(get_cleaned_dataframe(pd.DataFrame())), type(pd.DataFrame()))\n",
    "        self.assertEqual(type(vizualise_interesting_thing_1(pd.DataFrame())), type(None))\n",
    "        #self.assertEqual(type(build_a_model(pd.DataFrame())), type(None))\n",
    "        self.assertEqual(type(plot_my_model(pd.DataFrame())), type(None))\"\"\"\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "\n",
    "def main():\n",
    "    print(\"Start \\n\")\n",
    "\n",
    "\n",
    "    ########## GET DATA FROM SOURCES\n",
    "    \n",
    "    # Scrapp\n",
    "    df_scrap = get_scrap_dataframe()\n",
    "    \n",
    "    # Api\n",
    "    df_api = get_api_dataframe()\n",
    "    \n",
    "    # Import\n",
    "    df_import = get_import_dataframe()\n",
    "    \n",
    "    #########\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########## DATA JOINING\n",
    "    \n",
    "    #df_joined = get_joined_dataframe(df_scrap, df_api, df_import)\n",
    "    \n",
    "    \n",
    "    ########\n",
    "    \n",
    "    \n",
    "    ########## DATA CLEANING (after or before join depending on data)\n",
    "    \n",
    "    #df_cleaned = get_cleaned_dataframe(df_joined)\n",
    "    df_cleaned = get_cleaned_import_dataframe(df_import)\n",
    "    \n",
    "    #########\n",
    "    \n",
    "    \n",
    "    ########## DATA VISUALISATION\n",
    "    \n",
    "    vizualise_interesting_thing_1(df_cleaned)\n",
    "    #visualise_scatter(df_cleaned)\n",
    "    \n",
    "    #########\n",
    "    \n",
    "    ########## DATA EXPLOTATION\n",
    "    \n",
    "    # Use model to predict\n",
    "    \n",
    "    model_built = build_a_model(df_cleaned)\n",
    "    \n",
    "    plot_my_model(model_built)\n",
    "    \n",
    "    \n",
    "    #########\n",
    "    \n",
    "\n",
    "    print(\"End \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
